{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"hidden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_gpt(document) :\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"This is a student peer review environment. Your task is to generate a proper feedback from the instructor to the given student's assignment. So the input is the original assignment, the output is the instructor's feedback.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This is the assignment:\" + document \n",
    "      },\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=768,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DF = pd.read_csv('AutoReview.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "39335\n",
      "255ignored because too long\n",
      "256\n",
      "11331\n",
      "256ignored because too long\n",
      "257\n",
      "10402\n",
      "258\n",
      "5341\n",
      "259\n",
      "16830\n",
      "259ignored because too long\n",
      "260\n",
      "5289\n",
      "261\n",
      "4262\n",
      "262\n",
      "5568\n",
      "263\n",
      "8954\n",
      "264\n",
      "8936\n",
      "265\n",
      "15471\n",
      "265ignored because too long\n",
      "266\n",
      "10666\n",
      "267\n",
      "6672\n",
      "268\n",
      "14592\n",
      "268ignored because too long\n",
      "269\n",
      "3497\n",
      "270\n",
      "6926\n",
      "271\n",
      "6113\n",
      "272\n",
      "5979\n",
      "273\n",
      "7945\n",
      "274\n",
      "6950\n",
      "275\n",
      "7537\n",
      "276\n",
      "4973\n",
      "277\n",
      "3238\n",
      "278\n",
      "5076\n",
      "279\n",
      "3510\n",
      "280\n",
      "8108\n",
      "281\n",
      "4104\n",
      "282\n",
      "6353\n",
      "283\n",
      "4426\n",
      "284\n",
      "5337\n",
      "285\n",
      "11662\n",
      "285ignored because too long\n",
      "286\n",
      "3289\n",
      "287\n",
      "1836\n",
      "288\n",
      "5275\n",
      "289\n",
      "7410\n",
      "290\n",
      "6132\n",
      "291\n",
      "10012\n",
      "292\n",
      "12040\n",
      "292ignored because too long\n",
      "293\n",
      "10099\n",
      "294\n",
      "13144\n",
      "294ignored because too long\n",
      "295\n",
      "406\n",
      "296\n",
      "3563\n",
      "297\n",
      "2509\n",
      "298\n",
      "8869\n",
      "299\n",
      "5022\n",
      "300\n",
      "12812\n",
      "300ignored because too long\n",
      "301\n",
      "4433\n",
      "302\n",
      "8029\n",
      "303\n",
      "6479\n",
      "304\n",
      "7049\n",
      "305\n",
      "11349\n",
      "305ignored because too long\n",
      "306\n",
      "3669\n",
      "307\n",
      "2652\n",
      "308\n",
      "3727\n",
      "309\n",
      "4840\n",
      "310\n",
      "9515\n",
      "311\n",
      "4367\n",
      "312\n",
      "6597\n",
      "313\n",
      "4724\n",
      "314\n",
      "7192\n",
      "315\n",
      "2847\n",
      "316\n",
      "10896\n",
      "317\n",
      "6248\n",
      "318\n",
      "7403\n",
      "319\n",
      "5369\n",
      "320\n",
      "5258\n",
      "321\n",
      "7690\n",
      "322\n",
      "8056\n",
      "323\n",
      "4549\n",
      "324\n",
      "5441\n",
      "325\n",
      "9993\n",
      "326\n",
      "4672\n",
      "327\n",
      "12034\n",
      "327ignored because too long\n",
      "328\n",
      "14431\n",
      "328ignored because too long\n",
      "329\n",
      "19074\n",
      "329ignored because too long\n",
      "330\n",
      "8076\n",
      "331\n",
      "14922\n",
      "331ignored because too long\n",
      "332\n",
      "5340\n",
      "333\n",
      "10234\n",
      "334\n",
      "5521\n",
      "335\n",
      "4572\n",
      "336\n",
      "5937\n",
      "337\n",
      "8603\n",
      "338\n",
      "7657\n",
      "339\n",
      "6660\n",
      "340\n",
      "4164\n",
      "341\n",
      "19474\n",
      "341ignored because too long\n",
      "342\n",
      "12873\n",
      "342ignored because too long\n",
      "343\n",
      "8992\n",
      "344\n",
      "3032\n",
      "345\n",
      "8950\n",
      "346\n",
      "4829\n",
      "347\n",
      "7730\n",
      "348\n",
      "4369\n",
      "349\n",
      "11685\n",
      "349ignored because too long\n",
      "350\n",
      "5925\n",
      "351\n",
      "5574\n",
      "352\n",
      "3127\n",
      "353\n",
      "3917\n",
      "354\n",
      "3763\n",
      "355\n",
      "10839\n",
      "356\n",
      "10100\n",
      "357\n",
      "6830\n",
      "358\n",
      "13833\n",
      "358ignored because too long\n",
      "359\n",
      "5929\n",
      "360\n",
      "2691\n",
      "361\n",
      "7392\n",
      "362\n",
      "7524\n",
      "363\n",
      "6822\n",
      "364\n",
      "9493\n",
      "365\n",
      "3598\n",
      "366\n",
      "6113\n",
      "367\n",
      "3323\n",
      "368\n",
      "4215\n",
      "369\n",
      "6156\n",
      "370\n",
      "1431\n",
      "371\n",
      "3875\n",
      "372\n",
      "11672\n",
      "372ignored because too long\n",
      "373\n",
      "5172\n",
      "374\n",
      "7257\n",
      "375\n",
      "3445\n",
      "376\n",
      "5680\n",
      "377\n",
      "14086\n",
      "377ignored because too long\n",
      "378\n",
      "5408\n",
      "379\n",
      "2373\n",
      "380\n",
      "4942\n",
      "381\n",
      "5441\n",
      "382\n",
      "2478\n",
      "383\n",
      "5538\n",
      "384\n",
      "9562\n",
      "385\n",
      "4054\n",
      "386\n",
      "6864\n",
      "387\n",
      "4394\n",
      "388\n",
      "19885\n",
      "388ignored because too long\n",
      "389\n",
      "9134\n",
      "390\n",
      "3582\n",
      "391\n",
      "7493\n",
      "392\n",
      "17179\n",
      "392ignored because too long\n",
      "393\n",
      "3625\n",
      "394\n",
      "6383\n",
      "395\n",
      "9124\n",
      "396\n",
      "12575\n",
      "396ignored because too long\n",
      "397\n",
      "6766\n",
      "398\n",
      "7414\n",
      "399\n",
      "1544\n",
      "400\n",
      "9385\n",
      "401\n",
      "5695\n",
      "402\n",
      "7094\n",
      "403\n",
      "5141\n",
      "404\n",
      "9882\n",
      "405\n",
      "7871\n",
      "406\n",
      "12821\n",
      "406ignored because too long\n",
      "407\n",
      "10946\n",
      "408\n",
      "6033\n",
      "409\n",
      "4240\n",
      "410\n",
      "9891\n",
      "411\n",
      "2448\n",
      "412\n",
      "6749\n",
      "413\n",
      "5138\n",
      "414\n",
      "2624\n",
      "415\n",
      "5304\n",
      "416\n",
      "9505\n",
      "417\n",
      "7567\n",
      "418\n",
      "5507\n",
      "419\n",
      "3209\n",
      "420\n",
      "5205\n",
      "421\n",
      "2643\n",
      "422\n",
      "6074\n",
      "423\n",
      "9788\n",
      "424\n",
      "12278\n",
      "424ignored because too long\n",
      "425\n",
      "7565\n",
      "426\n",
      "3909\n",
      "427\n",
      "5171\n",
      "428\n",
      "5140\n",
      "429\n",
      "2968\n",
      "430\n",
      "1644\n",
      "431\n",
      "4445\n",
      "432\n",
      "9030\n",
      "433\n",
      "6502\n",
      "434\n",
      "7574\n",
      "435\n",
      "7775\n",
      "436\n",
      "6876\n",
      "437\n",
      "10252\n",
      "438\n",
      "7458\n",
      "439\n",
      "7054\n",
      "440\n",
      "15852\n",
      "440ignored because too long\n",
      "441\n",
      "4195\n",
      "442\n",
      "11672\n",
      "442ignored because too long\n",
      "443\n",
      "1244\n",
      "444\n",
      "1612\n",
      "445\n",
      "14532\n",
      "445ignored because too long\n",
      "446\n",
      "3112\n",
      "447\n",
      "2175\n",
      "448\n",
      "4434\n",
      "449\n",
      "5009\n",
      "450\n",
      "13031\n",
      "450ignored because too long\n",
      "451\n",
      "9410\n",
      "452\n",
      "5764\n",
      "453\n",
      "6972\n",
      "454\n",
      "11448\n",
      "454ignored because too long\n",
      "455\n",
      "2085\n",
      "456\n",
      "8498\n",
      "457\n",
      "11703\n",
      "457ignored because too long\n",
      "458\n",
      "16856\n",
      "458ignored because too long\n",
      "459\n",
      "5952\n",
      "460\n",
      "6330\n",
      "461\n",
      "5694\n",
      "462\n",
      "7263\n",
      "463\n",
      "7793\n",
      "464\n",
      "6774\n",
      "465\n",
      "8389\n",
      "466\n",
      "6293\n",
      "467\n",
      "1294\n",
      "468\n",
      "8584\n",
      "469\n",
      "13463\n",
      "469ignored because too long\n",
      "470\n",
      "7469\n",
      "471\n",
      "10174\n",
      "472\n",
      "12403\n",
      "472ignored because too long\n",
      "473\n",
      "7332\n",
      "474\n",
      "6151\n",
      "475\n",
      "4633\n",
      "476\n",
      "2364\n",
      "477\n",
      "3715\n",
      "478\n",
      "14719\n",
      "478ignored because too long\n",
      "479\n",
      "4756\n",
      "480\n",
      "11136\n",
      "480ignored because too long\n",
      "481\n",
      "11925\n",
      "481ignored because too long\n",
      "482\n",
      "9931\n",
      "483\n",
      "8001\n"
     ]
    }
   ],
   "source": [
    "# for the next time we could start from the number 236 to the remaining len. \n",
    "# due to run out of the balances, the model stops at 235.\n",
    "# the strlen > 11000 has been dropped, to avoid the restriction of rates.\n",
    "# one more cheaper way is to shorten the length, like summarize the original documents to save some # tokens. May consider this.\n",
    "# the following index starts from 1 !!!!!!\n",
    "# 244 too long 34164, ignored now, the same 255, 256, 259, 265, 268, 285, 292, 294, 300, 305, 327, 328, 329, 331, 341, 342, 349, 358, 372, 377, 388, 392, 396, 406, 424, 440, 442, 445, 450, 454, 457, 458, 469, 472, 478, 480, 481, 482\n",
    "# 245 dropped accidentally. need to redo 235-255 (21) works.\n",
    "\n",
    "# There are 3 pickle files dumped: gpt_raw is from 0 to 234 (235 total), after256 is from 257 to 483 (227 total, dropped some documents because they are too long)\n",
    "res_L = [] \n",
    "cnt = 254\n",
    "cnt_dropped = 0\n",
    "for i in DF['document'][255:] :\n",
    "    cnt = cnt + 1\n",
    "    print(cnt)\n",
    "    print(len(i))\n",
    "    if len(i)>11000 :\n",
    "        print(str(cnt)+ 'ignored because too long')\n",
    "        cnt_dropped = cnt_dropped + 1\n",
    "        continue\n",
    "    # response = send_to_gpt(i)\n",
    "    # time.sleep(1)\n",
    "    # res_L.append(response)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7870"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DF['document'][254])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9151\n",
      "2\n",
      "10543\n",
      "3\n",
      "6792\n",
      "4\n",
      "15742\n",
      "5\n",
      "22444\n",
      "6\n",
      "7684\n",
      "7\n",
      "5597\n",
      "8\n",
      "6163\n",
      "9\n",
      "7049\n",
      "10\n",
      "2547\n",
      "11\n",
      "6396\n",
      "12\n",
      "6956\n",
      "13\n",
      "10514\n",
      "14\n",
      "7552\n",
      "15\n",
      "9124\n",
      "16\n",
      "6326\n",
      "17\n",
      "9315\n",
      "18\n",
      "7025\n",
      "19\n",
      "4371\n",
      "20\n",
      "8528\n",
      "21\n",
      "9700\n",
      "22\n",
      "4145\n",
      "23\n",
      "10195\n",
      "24\n",
      "2729\n",
      "25\n",
      "7074\n",
      "26\n",
      "4237\n",
      "27\n",
      "5992\n",
      "28\n",
      "3766\n",
      "29\n",
      "5790\n",
      "30\n",
      "7760\n",
      "31\n",
      "9342\n",
      "32\n",
      "8588\n",
      "33\n",
      "8480\n",
      "34\n",
      "3721\n",
      "35\n",
      "5627\n",
      "36\n",
      "15415\n",
      "37\n",
      "5039\n",
      "38\n",
      "6759\n",
      "39\n",
      "1742\n",
      "40\n",
      "9034\n",
      "41\n",
      "3442\n",
      "42\n",
      "16672\n",
      "43\n",
      "11286\n",
      "44\n",
      "2715\n",
      "45\n",
      "8763\n",
      "46\n",
      "7471\n",
      "47\n",
      "13491\n",
      "48\n",
      "9095\n",
      "49\n",
      "10600\n",
      "50\n",
      "10425\n",
      "51\n",
      "13240\n",
      "52\n",
      "8069\n",
      "53\n",
      "11985\n",
      "54\n",
      "12739\n",
      "55\n",
      "7700\n",
      "56\n",
      "8011\n",
      "57\n",
      "3885\n",
      "58\n",
      "4196\n",
      "59\n",
      "5904\n",
      "60\n",
      "10433\n",
      "61\n",
      "8360\n",
      "62\n",
      "9396\n",
      "63\n",
      "3311\n",
      "64\n",
      "14532\n",
      "65\n",
      "3076\n",
      "66\n",
      "5408\n",
      "67\n",
      "3413\n",
      "68\n",
      "4508\n",
      "69\n",
      "17825\n",
      "70\n",
      "3084\n",
      "71\n",
      "17107\n",
      "72\n",
      "6288\n",
      "73\n",
      "2795\n",
      "74\n",
      "5736\n",
      "75\n",
      "6396\n",
      "76\n",
      "7550\n",
      "77\n",
      "5737\n",
      "78\n",
      "7193\n",
      "79\n",
      "4977\n",
      "80\n",
      "4778\n",
      "81\n",
      "13031\n",
      "82\n",
      "5857\n",
      "83\n",
      "7213\n",
      "84\n",
      "10607\n",
      "85\n",
      "12749\n",
      "86\n",
      "6900\n",
      "87\n",
      "7459\n",
      "88\n",
      "7227\n",
      "89\n",
      "3308\n",
      "90\n",
      "9349\n",
      "91\n",
      "14705\n",
      "92\n",
      "2849\n",
      "93\n",
      "4973\n",
      "94\n",
      "7038\n",
      "95\n",
      "9495\n",
      "96\n",
      "10713\n",
      "97\n",
      "10885\n",
      "98\n",
      "2615\n",
      "99\n",
      "4620\n",
      "100\n",
      "3777\n",
      "101\n",
      "4348\n",
      "102\n",
      "4740\n",
      "103\n",
      "15166\n",
      "104\n",
      "5297\n",
      "105\n",
      "12721\n",
      "106\n",
      "3045\n",
      "107\n",
      "11364\n",
      "108\n",
      "9681\n",
      "109\n",
      "6154\n",
      "110\n",
      "2408\n",
      "111\n",
      "3746\n",
      "112\n",
      "8661\n",
      "113\n",
      "9594\n",
      "114\n",
      "4969\n",
      "115\n",
      "6310\n",
      "116\n",
      "2820\n",
      "117\n",
      "7705\n",
      "118\n",
      "3443\n",
      "119\n",
      "6193\n",
      "120\n",
      "2551\n",
      "121\n",
      "5693\n",
      "122\n",
      "6192\n",
      "123\n",
      "10407\n",
      "124\n",
      "31183\n",
      "125\n",
      "2588\n",
      "126\n",
      "12314\n",
      "127\n",
      "8987\n",
      "128\n",
      "5895\n",
      "129\n",
      "5687\n",
      "130\n",
      "10200\n",
      "131\n",
      "7576\n",
      "132\n",
      "18718\n",
      "133\n",
      "7197\n",
      "134\n",
      "11890\n",
      "135\n",
      "4633\n",
      "136\n",
      "7228\n",
      "137\n",
      "6127\n",
      "138\n",
      "7264\n",
      "139\n",
      "12480\n",
      "140\n",
      "9937\n",
      "141\n",
      "6820\n",
      "142\n",
      "4726\n",
      "143\n",
      "2262\n",
      "144\n",
      "6109\n",
      "145\n",
      "3264\n",
      "146\n",
      "6163\n",
      "147\n",
      "8719\n",
      "148\n",
      "4557\n",
      "149\n",
      "4898\n",
      "150\n",
      "4118\n",
      "151\n",
      "5875\n",
      "152\n",
      "3173\n",
      "153\n",
      "5538\n",
      "154\n",
      "5218\n",
      "155\n",
      "3250\n",
      "156\n",
      "3612\n",
      "157\n",
      "2998\n",
      "158\n",
      "7500\n",
      "159\n",
      "7370\n",
      "160\n",
      "2144\n",
      "161\n",
      "14392\n",
      "162\n",
      "2927\n",
      "163\n",
      "3811\n",
      "164\n",
      "7794\n",
      "165\n",
      "6293\n",
      "166\n",
      "5111\n",
      "167\n",
      "5753\n",
      "168\n",
      "7776\n",
      "169\n",
      "8181\n",
      "170\n",
      "8305\n",
      "171\n",
      "6757\n",
      "172\n",
      "5222\n",
      "173\n",
      "3209\n",
      "174\n",
      "2735\n",
      "175\n",
      "3525\n",
      "176\n",
      "3587\n",
      "177\n",
      "7012\n",
      "178\n",
      "12669\n",
      "179\n",
      "9957\n",
      "180\n",
      "18795\n",
      "181\n",
      "5079\n",
      "182\n",
      "11523\n",
      "183\n",
      "9181\n",
      "184\n",
      "12451\n",
      "185\n",
      "4376\n",
      "186\n",
      "9052\n",
      "187\n",
      "9246\n",
      "188\n",
      "6428\n",
      "189\n",
      "7511\n",
      "190\n",
      "3206\n",
      "191\n",
      "9190\n",
      "192\n",
      "13456\n",
      "193\n",
      "2475\n",
      "194\n",
      "10020\n",
      "195\n",
      "4290\n",
      "196\n",
      "1228\n",
      "197\n",
      "6576\n",
      "198\n",
      "7669\n",
      "199\n",
      "18038\n",
      "200\n",
      "6880\n",
      "201\n",
      "7964\n",
      "202\n",
      "9655\n",
      "203\n",
      "6424\n",
      "204\n",
      "5961\n",
      "205\n",
      "5277\n",
      "206\n",
      "6496\n",
      "207\n",
      "5476\n",
      "208\n",
      "10017\n",
      "209\n",
      "4045\n",
      "210\n",
      "9132\n",
      "211\n",
      "8298\n",
      "212\n",
      "8391\n",
      "213\n",
      "5294\n",
      "214\n",
      "11567\n",
      "215\n",
      "8257\n",
      "216\n",
      "5684\n",
      "217\n",
      "4584\n",
      "218\n",
      "3913\n",
      "219\n",
      "6984\n",
      "220\n",
      "5718\n",
      "221\n",
      "7697\n",
      "222\n",
      "11030\n",
      "223\n",
      "6772\n",
      "224\n",
      "6216\n",
      "225\n",
      "9306\n",
      "226\n",
      "5380\n",
      "227\n",
      "4270\n",
      "228\n",
      "6036\n",
      "229\n",
      "11711\n",
      "230\n",
      "8798\n",
      "231\n",
      "4620\n",
      "232\n",
      "9171\n",
      "233\n",
      "5776\n",
      "234\n",
      "9325\n",
      "235\n",
      "9240\n",
      "236\n",
      "7678\n",
      "237\n",
      "4646\n",
      "238\n",
      "11819\n",
      "239\n",
      "4046\n",
      "240\n",
      "9405\n",
      "241\n",
      "6106\n",
      "242\n",
      "16484\n",
      "243\n",
      "11088\n",
      "244\n",
      "34164\n",
      "245\n",
      "2751\n",
      "246\n",
      "7051\n",
      "247\n",
      "7953\n",
      "248\n",
      "5944\n",
      "249\n",
      "2891\n",
      "250\n",
      "10580\n",
      "251\n",
      "7354\n",
      "252\n",
      "4370\n",
      "253\n",
      "4147\n",
      "254\n",
      "6571\n",
      "255\n",
      "7870\n",
      "256\n",
      "39335\n",
      "257\n",
      "11331\n",
      "258\n",
      "10402\n",
      "259\n",
      "5341\n",
      "260\n",
      "16830\n",
      "261\n",
      "5289\n",
      "262\n",
      "4262\n",
      "263\n",
      "5568\n",
      "264\n",
      "8954\n",
      "265\n",
      "8936\n",
      "266\n",
      "15471\n",
      "267\n",
      "10666\n",
      "268\n",
      "6672\n",
      "269\n",
      "14592\n",
      "270\n",
      "3497\n",
      "271\n",
      "6926\n",
      "272\n",
      "6113\n",
      "273\n",
      "5979\n",
      "274\n",
      "7945\n",
      "275\n",
      "6950\n",
      "276\n",
      "7537\n",
      "277\n",
      "4973\n",
      "278\n",
      "3238\n",
      "279\n",
      "5076\n",
      "280\n",
      "3510\n",
      "281\n",
      "8108\n",
      "282\n",
      "4104\n",
      "283\n",
      "6353\n",
      "284\n",
      "4426\n",
      "285\n",
      "5337\n",
      "286\n",
      "11662\n",
      "287\n",
      "3289\n",
      "288\n",
      "1836\n",
      "289\n",
      "5275\n",
      "290\n",
      "7410\n",
      "291\n",
      "6132\n",
      "292\n",
      "10012\n",
      "293\n",
      "12040\n",
      "294\n",
      "10099\n",
      "295\n",
      "13144\n",
      "296\n",
      "406\n",
      "297\n",
      "3563\n",
      "298\n",
      "2509\n",
      "299\n",
      "8869\n",
      "300\n",
      "5022\n",
      "301\n",
      "12812\n",
      "302\n",
      "4433\n",
      "303\n",
      "8029\n",
      "304\n",
      "6479\n",
      "305\n",
      "7049\n",
      "306\n",
      "11349\n",
      "307\n",
      "3669\n",
      "308\n",
      "2652\n",
      "309\n",
      "3727\n",
      "310\n",
      "4840\n",
      "311\n",
      "9515\n",
      "312\n",
      "4367\n",
      "313\n",
      "6597\n",
      "314\n",
      "4724\n",
      "315\n",
      "7192\n",
      "316\n",
      "2847\n",
      "317\n",
      "10896\n",
      "318\n",
      "6248\n",
      "319\n",
      "7403\n",
      "320\n",
      "5369\n",
      "321\n",
      "5258\n",
      "322\n",
      "7690\n",
      "323\n",
      "8056\n",
      "324\n",
      "4549\n",
      "325\n",
      "5441\n",
      "326\n",
      "9993\n",
      "327\n",
      "4672\n",
      "328\n",
      "12034\n",
      "329\n",
      "14431\n",
      "330\n",
      "19074\n",
      "331\n",
      "8076\n",
      "332\n",
      "14922\n",
      "333\n",
      "5340\n",
      "334\n",
      "10234\n",
      "335\n",
      "5521\n",
      "336\n",
      "4572\n",
      "337\n",
      "5937\n",
      "338\n",
      "8603\n",
      "339\n",
      "7657\n",
      "340\n",
      "6660\n",
      "341\n",
      "4164\n",
      "342\n",
      "19474\n",
      "343\n",
      "12873\n",
      "344\n",
      "8992\n",
      "345\n",
      "3032\n",
      "346\n",
      "8950\n",
      "347\n",
      "4829\n",
      "348\n",
      "7730\n",
      "349\n",
      "4369\n",
      "350\n",
      "11685\n",
      "351\n",
      "5925\n",
      "352\n",
      "5574\n",
      "353\n",
      "3127\n",
      "354\n",
      "3917\n",
      "355\n",
      "3763\n",
      "356\n",
      "10839\n",
      "357\n",
      "10100\n",
      "358\n",
      "6830\n",
      "359\n",
      "13833\n",
      "360\n",
      "5929\n",
      "361\n",
      "2691\n",
      "362\n",
      "7392\n",
      "363\n",
      "7524\n",
      "364\n",
      "6822\n",
      "365\n",
      "9493\n",
      "366\n",
      "3598\n",
      "367\n",
      "6113\n",
      "368\n",
      "3323\n",
      "369\n",
      "4215\n",
      "370\n",
      "6156\n",
      "371\n",
      "1431\n",
      "372\n",
      "3875\n",
      "373\n",
      "11672\n",
      "374\n",
      "5172\n",
      "375\n",
      "7257\n",
      "376\n",
      "3445\n",
      "377\n",
      "5680\n",
      "378\n",
      "14086\n",
      "379\n",
      "5408\n",
      "380\n",
      "2373\n",
      "381\n",
      "4942\n",
      "382\n",
      "5441\n",
      "383\n",
      "2478\n",
      "384\n",
      "5538\n",
      "385\n",
      "9562\n",
      "386\n",
      "4054\n",
      "387\n",
      "6864\n",
      "388\n",
      "4394\n",
      "389\n",
      "19885\n",
      "390\n",
      "9134\n",
      "391\n",
      "3582\n",
      "392\n",
      "7493\n",
      "393\n",
      "17179\n",
      "394\n",
      "3625\n",
      "395\n",
      "6383\n",
      "396\n",
      "9124\n",
      "397\n",
      "12575\n",
      "398\n",
      "6766\n",
      "399\n",
      "7414\n",
      "400\n",
      "1544\n",
      "401\n",
      "9385\n",
      "402\n",
      "5695\n",
      "403\n",
      "7094\n",
      "404\n",
      "5141\n",
      "405\n",
      "9882\n",
      "406\n",
      "7871\n",
      "407\n",
      "12821\n",
      "408\n",
      "10946\n",
      "409\n",
      "6033\n",
      "410\n",
      "4240\n",
      "411\n",
      "9891\n",
      "412\n",
      "2448\n",
      "413\n",
      "6749\n",
      "414\n",
      "5138\n",
      "415\n",
      "2624\n",
      "416\n",
      "5304\n",
      "417\n",
      "9505\n",
      "418\n",
      "7567\n",
      "419\n",
      "5507\n",
      "420\n",
      "3209\n",
      "421\n",
      "5205\n",
      "422\n",
      "2643\n",
      "423\n",
      "6074\n",
      "424\n",
      "9788\n",
      "425\n",
      "12278\n",
      "426\n",
      "7565\n",
      "427\n",
      "3909\n",
      "428\n",
      "5171\n",
      "429\n",
      "5140\n",
      "430\n",
      "2968\n",
      "431\n",
      "1644\n",
      "432\n",
      "4445\n",
      "433\n",
      "9030\n",
      "434\n",
      "6502\n",
      "435\n",
      "7574\n",
      "436\n",
      "7775\n",
      "437\n",
      "6876\n",
      "438\n",
      "10252\n",
      "439\n",
      "7458\n",
      "440\n",
      "7054\n",
      "441\n",
      "15852\n",
      "442\n",
      "4195\n",
      "443\n",
      "11672\n",
      "444\n",
      "1244\n",
      "445\n",
      "1612\n",
      "446\n",
      "14532\n",
      "447\n",
      "3112\n",
      "448\n",
      "2175\n",
      "449\n",
      "4434\n",
      "450\n",
      "5009\n",
      "451\n",
      "13031\n",
      "452\n",
      "9410\n",
      "453\n",
      "5764\n",
      "454\n",
      "6972\n",
      "455\n",
      "11448\n",
      "456\n",
      "2085\n",
      "457\n",
      "8498\n",
      "458\n",
      "11703\n",
      "459\n",
      "16856\n",
      "460\n",
      "5952\n",
      "461\n",
      "6330\n",
      "462\n",
      "5694\n",
      "463\n",
      "7263\n",
      "464\n",
      "7793\n",
      "465\n",
      "6774\n",
      "466\n",
      "8389\n",
      "467\n",
      "6293\n",
      "468\n",
      "1294\n",
      "469\n",
      "8584\n",
      "470\n",
      "13463\n",
      "471\n",
      "7469\n",
      "472\n",
      "10174\n",
      "473\n",
      "12403\n",
      "474\n",
      "7332\n",
      "475\n",
      "6151\n",
      "476\n",
      "4633\n",
      "477\n",
      "2364\n",
      "478\n",
      "3715\n",
      "479\n",
      "14719\n",
      "480\n",
      "4756\n",
      "481\n",
      "11136\n",
      "482\n",
      "11925\n",
      "483\n",
      "9931\n",
      "484\n",
      "8001\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in DF['document'] :\n",
    "    cnt = cnt + 1\n",
    "    print(cnt)\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_131736/76939049.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_L\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "res_L[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_from_gpt_after256_raw.pkl', 'wb') as f:\n",
    "    pkl.dump(res_L, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pkl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15245/249246887.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'response_from_gpt_raw.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pkl' is not defined"
     ]
    }
   ],
   "source": [
    "with open('response_from_gpt_raw.pkl', 'rb') as f:\n",
    "    L = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-83DzQQ5qz1wnQEOC6RU5t4mLxqFbP at 0x7fb2914ade00> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"You have done an excellent job explaining the complexities of the project and detailing the work you and your team have done. The description of the initial state of the code and the problems it featured was clear and well-organized. Your approach to refactoring was systematic and you provided illustrative and relevant examples which make it easier to understand the task that you were handling.\\n\\nThe breakdown of dividing the assignment_creation_spec.rb file into multiple specific ones, each with its own responsibility, was a great step towards better code organization. I also appreciate how you listed down the extensive list of test cases.\\n\\nYou were thoughtful about justifying which issues flagged by code climate were necessary to refactor and which ones weren't, this shows an understanding of practical programming considerations beyond simple adherence to a tool's recommendations. It was good seeing that you decided not to change DateTime to Date or Time, since it would require a lot of work outside of the defined scope of the project.\\n\\nIt is impressive that you were able to increase overall test coverage by more than 28% bringing it to a total of 52.334%. This represents a significant improvement in the overall robustness and maintainability of the Expertiza codebase and your team deserves to be commended for this.\\n\\nIn regards to future improvements, providing more context to the reader about terms such as DRY would be helpful. Not all readers may be familiar with these terms. Also, unless it is an image displaying code, figures should nearly always be supplied with a caption describing what they are demonstrating. The report would have greatly benefited from more explanations of the code snippets and images included in your submission.\\n\\nThe assignment was thoroughly done, and you should pride yourself on the detail and quality of your work. Overall, your submission is comprehensive and clear. Well done!\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1695779452,\n",
       "  \"id\": \"chatcmpl-83DzQQ5qz1wnQEOC6RU5t4mLxqFbP\",\n",
       "  \"model\": \"gpt-4-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 356,\n",
       "    \"prompt_tokens\": 1864,\n",
       "    \"total_tokens\": 2220\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickled Data Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('response_from_gpt_raw.pkl', \"rb\") as f:\n",
    "    p1 = pkl.load(f)\n",
    "with open('response_from_gpt_after256_raw.pkl', \"rb\") as f:\n",
    "    p2 = pkl.load(f)\n",
    "len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropping_too_long_after256(DF) :\n",
    "    ret = []\n",
    "    for i in range(255, 484) :\n",
    "        if len(DF['document'][i]) > 11000 :\n",
    "            ret.append(i)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropping_too_long_after256(DF)\n",
    "len(dropping_too_long_after256(DF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped_index_L = dropping_too_long_after256(DF)\n",
    "dropped_2long_DF = DF.drop(dropped_index_L)\n",
    "dropped_2long_DF = dropped_2long_DF.drop([x for x in range(235, 255)])\n",
    "len(dropped_2long_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_copy = DF\n",
    "DF = dropped_2long_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DF_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 2 lists p1, p2 together\n",
    "L = p1 + p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_type = \"microsoft/deberta-xlarge-mnli\"\n",
    "model_lang = \"en\"\n",
    "bert_scorer = BERTScorer(model_type=model_type, lang=model_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You have done an excellent job explaining the complexities of the project and detailing the work you and your team have done. The description of the initial state of the code and the problems it featured was clear and well-organized. Your approach to refactoring was systematic and you provided illustrative and relevant examples which make it easier to understand the task that you were handling.\\n\\nThe breakdown of dividing the assignment_creation_spec.rb file into multiple specific ones, each with its own responsibility, was a great step towards better code organization. I also appreciate how you listed down the extensive list of test cases.\\n\\nYou were thoughtful about justifying which issues flagged by code climate were necessary to refactor and which ones weren't, this shows an understanding of practical programming considerations beyond simple adherence to a tool's recommendations. It was good seeing that you decided not to change DateTime to Date or Time, since it would require a lot of work outside of the defined scope of the project.\\n\\nIt is impressive that you were able to increase overall test coverage by more than 28% bringing it to a total of 52.334%. This represents a significant improvement in the overall robustness and maintainability of the Expertiza codebase and your team deserves to be commended for this.\\n\\nIn regards to future improvements, providing more context to the reader about terms such as DRY would be helpful. Not all readers may be familiar with these terms. Also, unless it is an image displaying code, figures should nearly always be supplied with a caption describing what they are demonstrating. The report would have greatly benefited from more explanations of the code snippets and images included in your submission.\\n\\nThe assignment was thoroughly done, and you should pride yourself on the detail and quality of your work. Overall, your submission is comprehensive and clear. Well done!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[0]['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_filtered_reqL = []\n",
    "for i in L :\n",
    "    response_filtered_reqL.append(i['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_L = DF['summary'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a thorough and comprehensive assignment submission. Your detailed understanding of Expertiza and its score calculation process is impressive. You have demonstrated an understanding of how the system works and have proposed a well-thought-out solution that could improve its efficiency. Your diagrams illustrate your ideas very well, and the use of visual aids is encouraged as it helps with understanding complex systems.\\n\\nFor improvement, there are few areas that need to be refined. While your explanation of the proposed methodology is thorough, it could be more concise. Try to eliminate any redundancies in your writing. Also, clear organization of the information presented could greatly improve the readability. Using headings and subheadings can help readers more easily understand your proposal and follow your thought process. \\n\\nFurthermore, there are several typographical errors and a few instances of awkward phrasing throughout your proposal. Proofreading your work and utilizing a grammar correction tool could help you catch these mistakes in the future. \\n\\nLastly, while you proposed to improve the score calculation functionality, you did not provide much detail about how this would be implemented and tested in practise. More information about the specifics of these changes would increase the practical feasibility of your proposal. \\n\\nIn conclusion, this is a well-researched project with a lot of potential. Keep up the good work and take these suggested improvements into your future works.\\n\\nGrade: B+'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_filtered_reqL[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The wiki was to the point and explains all the code changes they have made. Why they touched some parts of code and why they didn't touch the others. Their test plan mentions a list of all the test cases that are currently covered. It's not clear how that list is organized, and given that there are dozens of items, there needs to be a logical ordering to them. Also, they did not mention how they have tested it manually. Some reviewers complained that they did not know how to test it properly since their test plan did not cover it. \""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_L[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hdu5/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# This is necessary for the first time you use NLTK's sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "refs = sent_tokenize(feedback_L[0])\n",
    "cands = sent_tokenize(response_filtered_reqL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_BertScore(refs, cands) :\n",
    "    # Pairwise BERTScore (will get multiple scores for each candidate)\n",
    "    all_F1 = []\n",
    "    all_P = []\n",
    "    all_R = []\n",
    "    for cand in cands:\n",
    "        for ref in refs:\n",
    "            P, R, F1 = bert_scorer.score([cand], [ref])\n",
    "            all_P.append(P.item())\n",
    "            all_R.append(R.item())\n",
    "            all_F1.append(F1.item())\n",
    "\n",
    "    # Taking average of all scores for simplicity (or you could take the max)\n",
    "    average_F1 = sum(all_F1) / len(all_F1)\n",
    "    average_P = sum(all_P) / len(all_P)\n",
    "    average_R = sum(all_R) / len(all_R)\n",
    "    print(\"ave_F1=\"+str(average_F1))\n",
    "    print(\"ave_P=\"+str(average_P))\n",
    "    print(\"ave_R=\"+str(average_R))\n",
    "    return [average_F1, average_P, average_R] #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdu5/.local/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/home/hdu5/.local/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave_F1=0.5629226556047797\n",
      "ave_P=0.5666154126326243\n",
      "ave_R=0.5636037715400258\n",
      "ave_F1=0.5637062266469002\n",
      "ave_P=0.5765197845175862\n",
      "ave_R=0.5532652493566275\n",
      "ave_F1=0.49970869834606463\n",
      "ave_P=0.5254224882676051\n",
      "ave_R=0.4789347923718966\n",
      "ave_F1=0.5575408912622012\n",
      "ave_P=0.545118137047841\n",
      "ave_R=0.5747725382829324\n",
      "ave_F1=0.5557464464671082\n",
      "ave_P=0.5721214738570981\n",
      "ave_R=0.5524290005366007\n",
      "ave_F1=0.48448304043096657\n",
      "ave_P=0.4685226336998098\n",
      "ave_R=0.5071231968262616\n",
      "ave_F1=0.5330157621453205\n",
      "ave_P=0.5280143053581318\n",
      "ave_R=0.5468467337389787\n",
      "ave_F1=0.5395834119546981\n",
      "ave_P=0.5392259196156547\n",
      "ave_R=0.5428868652809233\n",
      "ave_F1=0.5757237290342648\n",
      "ave_P=0.5960065240661303\n",
      "ave_R=0.5694054881731669\n",
      "ave_F1=0.5492777693271637\n",
      "ave_P=0.5591106390953064\n",
      "ave_R=0.5497071146965027\n",
      "ave_F1=0.5234320169935623\n",
      "ave_P=0.5387763977050781\n",
      "ave_R=0.5132949103911718\n",
      "ave_F1=0.5803824795616997\n",
      "ave_P=0.5770128064685398\n",
      "ave_R=0.5879136343797048\n",
      "ave_F1=0.5344759254819818\n",
      "ave_P=0.5725478865206242\n",
      "ave_R=0.5162910227146413\n",
      "ave_F1=0.5556363640796571\n",
      "ave_P=0.581444691334452\n",
      "ave_R=0.5444604498999459\n",
      "ave_F1=0.5358558125220813\n",
      "ave_P=0.5742039227714906\n",
      "ave_R=0.5115693933688678\n",
      "ave_F1=0.6020918944302727\n",
      "ave_P=0.5814904439098695\n",
      "ave_R=0.6285135938840754\n",
      "ave_F1=0.5441944516803089\n",
      "ave_P=0.5311352732150179\n",
      "ave_R=0.5723893983583701\n",
      "ave_F1=0.5499612856656313\n",
      "ave_P=0.5575547913710276\n",
      "ave_R=0.5452076867222786\n",
      "ave_F1=0.5535255908034742\n",
      "ave_P=0.5626135035417974\n",
      "ave_R=0.5578164299950004\n",
      "ave_F1=0.5394441902637481\n",
      "ave_P=0.5642851034800211\n",
      "ave_R=0.5251002537707488\n",
      "ave_F1=0.49875548481941223\n",
      "ave_P=0.5592705450559917\n",
      "ave_R=0.46086450548548447\n",
      "ave_F1=0.5533090601949131\n",
      "ave_P=0.5462436226068759\n",
      "ave_R=0.5670386061948889\n",
      "ave_F1=0.5757939473787943\n",
      "ave_P=0.5918634176254273\n",
      "ave_R=0.5654661941528321\n",
      "ave_F1=0.5498406711746665\n",
      "ave_P=0.5774618643171647\n",
      "ave_R=0.5358315523932962\n",
      "ave_F1=0.5228094846392289\n",
      "ave_P=0.5518444577852885\n",
      "ave_R=0.5129719070899181\n",
      "ave_F1=0.5710081540875964\n",
      "ave_P=0.5892442940837808\n",
      "ave_R=0.568701426188151\n",
      "ave_F1=0.5515049790990525\n",
      "ave_P=0.5549701046252596\n",
      "ave_R=0.5612154451833256\n",
      "ave_F1=0.5764667852719625\n",
      "ave_P=0.5865765607357025\n",
      "ave_R=0.5711962048212688\n",
      "ave_F1=0.5582485028675624\n",
      "ave_P=0.5695949182623908\n",
      "ave_R=0.549590772816113\n",
      "ave_F1=0.5416568564433678\n",
      "ave_P=0.5339693035565171\n",
      "ave_R=0.5535046423182768\n",
      "ave_F1=0.5776135245958964\n",
      "ave_P=0.5915860342979431\n",
      "ave_R=0.5664017955462138\n",
      "ave_F1=0.5978480214657991\n",
      "ave_P=0.5774533735669177\n",
      "ave_R=0.6358548934045045\n",
      "ave_F1=0.5528439800990256\n",
      "ave_P=0.5612475209330258\n",
      "ave_R=0.5475414752175933\n",
      "ave_F1=0.5338762991130352\n",
      "ave_P=0.5766826922694842\n",
      "ave_R=0.5115407990912596\n",
      "ave_F1=0.5285667050629854\n",
      "ave_P=0.5134460820506016\n",
      "ave_R=0.5517818313091993\n",
      "ave_F1=0.6345478456753951\n",
      "ave_P=0.6028732267709879\n",
      "ave_R=0.6715591595723078\n",
      "ave_F1=0.5656365819952704\n",
      "ave_P=0.5516659204255451\n",
      "ave_R=0.5811069451949813\n",
      "ave_F1=0.5246138479560614\n",
      "ave_P=0.5754317821313938\n",
      "ave_R=0.4929439276456833\n",
      "ave_F1=0.5768020115792751\n",
      "ave_P=0.5641286455094814\n",
      "ave_R=0.5919204950332642\n",
      "ave_F1=0.5989967733621597\n",
      "ave_P=0.6231240183115005\n",
      "ave_R=0.5798961172501246\n",
      "ave_F1=0.5525858031378852\n",
      "ave_P=0.5537261380089654\n",
      "ave_R=0.5547851509518094\n",
      "ave_F1=0.553997694138895\n",
      "ave_P=0.5725544994337517\n",
      "ave_R=0.5388417434796953\n",
      "ave_F1=0.5386571175221241\n",
      "ave_P=0.5994534424760125\n",
      "ave_R=0.4990858155669588\n",
      "ave_F1=0.5559377459188303\n",
      "ave_P=0.5339997795720896\n",
      "ave_R=0.5842235663698779\n",
      "ave_F1=0.5508943295904568\n",
      "ave_P=0.5944591446646622\n",
      "ave_R=0.5248670168220997\n",
      "ave_F1=0.5518920611718605\n",
      "ave_P=0.590268285110079\n",
      "ave_R=0.5336170396928129\n",
      "ave_F1=0.535075014545804\n",
      "ave_P=0.5855973031785753\n",
      "ave_R=0.5020664608667768\n",
      "ave_F1=0.5578334340825677\n",
      "ave_P=0.5510440324433148\n",
      "ave_R=0.5688179391436279\n",
      "ave_F1=0.5333091473137891\n",
      "ave_P=0.5992930807449199\n",
      "ave_R=0.49117441199443956\n",
      "ave_F1=0.5699539968446523\n",
      "ave_P=0.57741511268776\n",
      "ave_R=0.5652292420884141\n",
      "ave_F1=0.5488922255113721\n",
      "ave_P=0.5918765043839812\n",
      "ave_R=0.5251981617882848\n",
      "ave_F1=0.5646420177072287\n",
      "ave_P=0.5417531337589025\n",
      "ave_R=0.5966683253645897\n",
      "ave_F1=0.5935046634837693\n",
      "ave_P=0.5887853322075862\n",
      "ave_R=0.6007066880371056\n",
      "ave_F1=0.5934601618693425\n",
      "ave_P=0.592692415530865\n",
      "ave_R=0.5975378096103668\n",
      "ave_F1=0.5720020942389965\n",
      "ave_P=0.5864944281056523\n",
      "ave_R=0.5624704202637076\n",
      "ave_F1=0.5756952312021029\n",
      "ave_P=0.5810177610034034\n",
      "ave_R=0.5757182407237235\n",
      "ave_F1=0.5321403629617927\n",
      "ave_P=0.5645933197236356\n",
      "ave_R=0.5213955909381678\n",
      "ave_F1=0.5254351410426592\n",
      "ave_P=0.5394446618462864\n",
      "ave_R=0.5189114958047867\n",
      "ave_F1=0.5777935108000581\n",
      "ave_P=0.5915610749613155\n",
      "ave_R=0.5758016441356052\n",
      "ave_F1=0.5818789688249429\n",
      "ave_P=0.6044525305430094\n",
      "ave_R=0.5628021694719791\n",
      "ave_F1=0.5611111416536219\n",
      "ave_P=0.5596018679001752\n",
      "ave_R=0.5676145911717615\n",
      "ave_F1=0.5747677306334178\n",
      "ave_P=0.5205714563528697\n",
      "ave_R=0.6451428969701131\n",
      "ave_F1=0.5223368811938498\n",
      "ave_P=0.5155506137344572\n",
      "ave_R=0.5332236205538113\n",
      "ave_F1=0.5382041342747517\n",
      "ave_P=0.5596476296583811\n",
      "ave_R=0.5295453308484493\n",
      "ave_F1=0.5209333240985871\n",
      "ave_P=0.5200530098544227\n",
      "ave_R=0.5229078663720025\n",
      "ave_F1=0.5156589647134145\n",
      "ave_P=0.5399688970711496\n",
      "ave_R=0.5116199337773852\n",
      "ave_F1=0.5492134493589401\n",
      "ave_P=0.5660817766189575\n",
      "ave_R=0.5462363463640213\n",
      "ave_F1=0.5675889505516916\n",
      "ave_P=0.5652184919232414\n",
      "ave_R=0.5815904988419442\n",
      "ave_F1=0.5854718110391072\n",
      "ave_P=0.5811183268115634\n",
      "ave_R=0.5915144689026333\n",
      "ave_F1=0.5805928512261465\n",
      "ave_P=0.5615081162406848\n",
      "ave_R=0.6045810224918219\n",
      "ave_F1=0.5311376727289624\n",
      "ave_P=0.5540678709745407\n",
      "ave_R=0.5236475411388609\n",
      "ave_F1=0.5386868176551965\n",
      "ave_P=0.5502197765387021\n",
      "ave_R=0.5294699187462146\n",
      "ave_F1=0.5284389011807494\n",
      "ave_P=0.5302204450408181\n",
      "ave_R=0.5354097999714234\n",
      "ave_F1=0.5376462975982577\n",
      "ave_P=0.5579474827973172\n",
      "ave_R=0.5319704794092104\n",
      "ave_F1=0.5561977336804073\n",
      "ave_P=0.545351209739844\n",
      "ave_R=0.5686750690142314\n",
      "ave_F1=0.5829142102350792\n",
      "ave_P=0.5978519382576147\n",
      "ave_R=0.5711083232114712\n",
      "ave_F1=0.5571719939693992\n",
      "ave_P=0.5830158236218087\n",
      "ave_R=0.5468145226254876\n",
      "ave_F1=0.5584480702877045\n",
      "ave_P=0.5824191133181255\n",
      "ave_R=0.549591429233551\n",
      "ave_F1=0.5600430231827956\n",
      "ave_P=0.5539337281997387\n",
      "ave_R=0.5668758772886716\n",
      "ave_F1=0.5270619883581444\n",
      "ave_P=0.5179601758718491\n",
      "ave_R=0.5401488112078773\n",
      "ave_F1=0.5401387981006077\n",
      "ave_P=0.5772309540485849\n",
      "ave_R=0.5208813000698479\n",
      "ave_F1=0.5389895608612135\n",
      "ave_P=0.5885111981747198\n",
      "ave_R=0.5103305683416479\n",
      "ave_F1=0.555985763669014\n",
      "ave_P=0.5473877441670213\n",
      "ave_R=0.5707339208040919\n",
      "ave_F1=0.5488031208515167\n",
      "ave_P=0.5773251201761397\n",
      "ave_R=0.5330046693745413\n",
      "ave_F1=0.5641741640865803\n",
      "ave_P=0.5671309041790664\n",
      "ave_R=0.5641839629970491\n",
      "ave_F1=0.5651302978847966\n",
      "ave_P=0.5681724873456088\n",
      "ave_R=0.5630403278451978\n",
      "ave_F1=0.5537562194195661\n",
      "ave_P=0.5460645034909248\n",
      "ave_R=0.5651613060723651\n",
      "ave_F1=0.5523960081736247\n",
      "ave_P=0.5348159515857697\n",
      "ave_R=0.5770426321029664\n",
      "ave_F1=0.5427991656156687\n",
      "ave_P=0.5674256842869979\n",
      "ave_R=0.5213840649678156\n",
      "ave_F1=0.5857760575082567\n",
      "ave_P=0.598214688676375\n",
      "ave_R=0.5772281520896487\n",
      "ave_F1=0.5721860770136118\n",
      "ave_P=0.5949760700265566\n",
      "ave_R=0.5531679795434078\n",
      "ave_F1=0.5093540032704671\n",
      "ave_P=0.4487075567245483\n",
      "ave_R=0.5970778544743855\n",
      "ave_F1=0.549415079795796\n",
      "ave_P=0.5904719972092173\n",
      "ave_R=0.5245107775149138\n",
      "ave_F1=0.5490919110320863\n",
      "ave_P=0.5984241089650563\n",
      "ave_R=0.5184282408583731\n",
      "ave_F1=0.5069379420841441\n",
      "ave_P=0.522957163698533\n",
      "ave_R=0.4946878254413605\n",
      "ave_F1=0.5956350273556179\n",
      "ave_P=0.6094185577498542\n",
      "ave_R=0.5844673507743412\n",
      "ave_F1=0.5518854012091955\n",
      "ave_P=0.5772947569688162\n",
      "ave_R=0.5299529414623976\n",
      "ave_F1=0.5606238286603581\n",
      "ave_P=0.5738575563512065\n",
      "ave_R=0.5590797730467536\n",
      "ave_F1=0.5912697082385421\n",
      "ave_P=0.6071137245744467\n",
      "ave_R=0.5794179821386933\n",
      "ave_F1=0.5288095392641567\n",
      "ave_P=0.5575218129725683\n",
      "ave_R=0.5185471120334807\n",
      "ave_F1=0.56502738289344\n",
      "ave_P=0.5898329806633484\n",
      "ave_R=0.5559473587916448\n",
      "ave_F1=0.5740695148706436\n",
      "ave_P=0.5686327261584145\n",
      "ave_R=0.587887588711012\n",
      "ave_F1=0.5795786259350953\n",
      "ave_P=0.6215607468728666\n",
      "ave_R=0.5533542927400565\n",
      "ave_F1=0.5597591844567081\n",
      "ave_P=0.5687664642668607\n",
      "ave_R=0.5540122938783545\n",
      "ave_F1=0.5663747221231461\n",
      "ave_P=0.5915064696754728\n",
      "ave_R=0.5535321386797087\n",
      "ave_F1=0.530800054470698\n",
      "ave_P=0.5642326048442295\n",
      "ave_R=0.5155664299215589\n",
      "ave_F1=0.5709936373556653\n",
      "ave_P=0.5673877770702044\n",
      "ave_R=0.5797534951319298\n",
      "ave_F1=0.5545357609433788\n",
      "ave_P=0.5676317140460014\n",
      "ave_R=0.543746103133474\n",
      "ave_F1=0.5501364558935166\n",
      "ave_P=0.568454937849726\n",
      "ave_R=0.5457738733717373\n",
      "ave_F1=0.5785960100945973\n",
      "ave_P=0.5866032725288755\n",
      "ave_R=0.573231360742024\n",
      "ave_F1=0.5342939061423143\n",
      "ave_P=0.5662864539772272\n",
      "ave_R=0.5091824171443781\n",
      "ave_F1=0.5614667932192484\n",
      "ave_P=0.5807532127946615\n",
      "ave_R=0.5560932209094366\n",
      "ave_F1=0.5508567134539286\n",
      "ave_P=0.5733507484197616\n",
      "ave_R=0.5440383136272431\n",
      "ave_F1=0.5483032137523463\n",
      "ave_P=0.574052036728388\n",
      "ave_R=0.5374048495734179\n",
      "ave_F1=0.531663863123327\n",
      "ave_P=0.5935414671034053\n",
      "ave_R=0.49061595356982685\n",
      "ave_F1=0.5777993209851093\n",
      "ave_P=0.5700344152939625\n",
      "ave_R=0.5903302408181704\n",
      "ave_F1=0.6089286630352339\n",
      "ave_P=0.5935941711068153\n",
      "ave_R=0.6268749932448069\n",
      "ave_F1=0.5500098310410977\n",
      "ave_P=0.5462540745735168\n",
      "ave_R=0.5558125525712967\n",
      "ave_F1=0.5643254403024912\n",
      "ave_P=0.5500022806227207\n",
      "ave_R=0.5834944881498814\n",
      "ave_F1=0.5512435482814908\n",
      "ave_P=0.5632136156782508\n",
      "ave_R=0.5446358481422067\n",
      "ave_F1=0.5352966649376828\n",
      "ave_P=0.6010591789432194\n",
      "ave_R=0.494140594549801\n",
      "ave_F1=0.5940803358597415\n",
      "ave_P=0.561648276235376\n",
      "ave_R=0.6366275788417884\n",
      "ave_F1=0.5684003502130508\n",
      "ave_P=0.5767521623108122\n",
      "ave_R=0.5651595400439369\n",
      "ave_F1=0.596974329756839\n",
      "ave_P=0.6233743854931423\n",
      "ave_R=0.5758925090943064\n",
      "ave_F1=0.5039718589361977\n",
      "ave_P=0.5154516696929932\n",
      "ave_R=0.4960118426996119\n",
      "ave_F1=0.5413727796914285\n",
      "ave_P=0.5601020222693159\n",
      "ave_R=0.5352509769431332\n",
      "ave_F1=0.5581749273197991\n",
      "ave_P=0.5609467178583145\n",
      "ave_R=0.5587286186360177\n",
      "ave_F1=0.5156883004307747\n",
      "ave_P=0.5568443813920021\n",
      "ave_R=0.4922264939546585\n",
      "ave_F1=0.5882315834363302\n",
      "ave_P=0.6220238844553629\n",
      "ave_R=0.5595802545547486\n",
      "ave_F1=0.5350113355865082\n",
      "ave_P=0.5752213432764014\n",
      "ave_R=0.5107260746881366\n",
      "ave_F1=0.572949286699295\n",
      "ave_P=0.5764778578281402\n",
      "ave_R=0.5737937960028648\n",
      "ave_F1=0.5477058944957597\n",
      "ave_P=0.5434398376515933\n",
      "ave_R=0.5550276366727692\n",
      "ave_F1=0.5303815846089963\n",
      "ave_P=0.5348807142840492\n",
      "ave_R=0.534779539262807\n",
      "ave_F1=0.5664690528064966\n",
      "ave_P=0.5609065452590585\n",
      "ave_R=0.5756652634590864\n",
      "ave_F1=0.533543316854371\n",
      "ave_P=0.5371211742361387\n",
      "ave_R=0.5398333138889737\n",
      "ave_F1=0.5281165776981248\n",
      "ave_P=0.5506310446394814\n",
      "ave_R=0.5175976602567567\n",
      "ave_F1=0.5600625525821339\n",
      "ave_P=0.602091091058471\n",
      "ave_R=0.5344984100623564\n",
      "ave_F1=0.5827337730872003\n",
      "ave_P=0.5725941179614318\n",
      "ave_R=0.5960786162238372\n",
      "ave_F1=0.5742619138497572\n",
      "ave_P=0.5634042616073902\n",
      "ave_R=0.5887317047669337\n",
      "ave_F1=0.602198691368103\n",
      "ave_P=0.593935826420784\n",
      "ave_R=0.6147090464830398\n",
      "ave_F1=0.5808342780385699\n",
      "ave_P=0.594788735466344\n",
      "ave_R=0.5699657584939685\n",
      "ave_F1=0.5294496898467724\n",
      "ave_P=0.5164499489160684\n",
      "ave_R=0.5461284036819751\n",
      "ave_F1=0.5792472723283266\n",
      "ave_P=0.6016184007912352\n",
      "ave_R=0.562013693545994\n",
      "ave_F1=0.5403509312974555\n",
      "ave_P=0.520340329834393\n",
      "ave_R=0.5806851424276829\n",
      "ave_F1=0.5375749261251518\n",
      "ave_P=0.5254800277096885\n",
      "ave_R=0.5605296263737338\n",
      "ave_F1=0.5517301964573562\n",
      "ave_P=0.5312686590477824\n",
      "ave_R=0.5778885358013213\n",
      "ave_F1=0.586545009083218\n",
      "ave_P=0.5894783390892877\n",
      "ave_R=0.5867005997233921\n",
      "ave_F1=0.5370003976251768\n",
      "ave_P=0.5644830614328384\n",
      "ave_R=0.5232416261797366\n",
      "ave_F1=0.5677140112966299\n",
      "ave_P=0.5809801388531923\n",
      "ave_R=0.5588946609447399\n",
      "ave_F1=0.5309277355670929\n",
      "ave_P=0.5556830763816833\n",
      "ave_R=0.5102832667529583\n",
      "ave_F1=0.5351793219645818\n",
      "ave_P=0.5268998295068741\n",
      "ave_R=0.5475945290591981\n",
      "ave_F1=0.5534691237935833\n",
      "ave_P=0.5519792864135667\n",
      "ave_R=0.5580323338508606\n",
      "ave_F1=0.5853844657540321\n",
      "ave_P=0.5947566234639713\n",
      "ave_R=0.5786022554550853\n",
      "ave_F1=0.5360676081149609\n",
      "ave_P=0.5459871346300299\n",
      "ave_R=0.5284109175592274\n",
      "ave_F1=0.6127430903060096\n",
      "ave_P=0.6219917578356606\n",
      "ave_R=0.6056144099150386\n",
      "ave_F1=0.565623783148252\n",
      "ave_P=0.5625948837170234\n",
      "ave_R=0.5710561814216467\n",
      "ave_F1=0.5473105153616737\n",
      "ave_P=0.5720754797552147\n",
      "ave_R=0.5268813476842993\n",
      "ave_F1=0.5409188983695847\n",
      "ave_P=0.5408482892172677\n",
      "ave_R=0.5464239578161921\n",
      "ave_F1=0.5999512216076255\n",
      "ave_P=0.5932619068771601\n",
      "ave_R=0.6084891026839614\n",
      "ave_F1=0.5689309291979846\n",
      "ave_P=0.5721782410846037\n",
      "ave_R=0.5678075709763695\n",
      "ave_F1=0.5608441979988762\n",
      "ave_P=0.5856046617031098\n",
      "ave_R=0.5508696703807168\n",
      "ave_F1=0.5437153540551662\n",
      "ave_P=0.552517730742693\n",
      "ave_R=0.5363549776375294\n",
      "ave_F1=0.6062954496592283\n",
      "ave_P=0.5958165004849434\n",
      "ave_R=0.6196102984249592\n",
      "ave_F1=0.49155610986053944\n",
      "ave_P=0.5162476059049368\n",
      "ave_R=0.4773743903885285\n",
      "ave_F1=0.5680054684635252\n",
      "ave_P=0.5527475997805595\n",
      "ave_R=0.5875633568502963\n",
      "ave_F1=0.5550951733002587\n",
      "ave_P=0.5695612217698779\n",
      "ave_R=0.5514745428448632\n",
      "ave_F1=0.5069249723202143\n",
      "ave_P=0.5240102884096977\n",
      "ave_R=0.4934713687652197\n",
      "ave_F1=0.5271602285405\n",
      "ave_P=0.5504082224021355\n",
      "ave_R=0.5179383674015602\n",
      "ave_F1=0.5651095185088318\n",
      "ave_P=0.5738768666734986\n",
      "ave_R=0.5611885562332714\n",
      "ave_F1=0.5470966804772616\n",
      "ave_P=0.5992861080914735\n",
      "ave_R=0.5148912329226732\n",
      "ave_F1=0.5881563136974971\n",
      "ave_P=0.5978391915559769\n",
      "ave_R=0.5814365476369858\n",
      "ave_F1=0.5793755189293907\n",
      "ave_P=0.5911223292350769\n",
      "ave_R=0.5723890683480671\n",
      "ave_F1=0.5216960928269795\n",
      "ave_P=0.5463700464793614\n",
      "ave_R=0.5011944089617048\n",
      "ave_F1=0.5711026840976307\n",
      "ave_P=0.5635446421802044\n",
      "ave_R=0.579999350543533\n",
      "ave_F1=0.5474713790416718\n",
      "ave_P=0.5713269656896591\n",
      "ave_R=0.5330666416883468\n",
      "ave_F1=0.5543253024419149\n",
      "ave_P=0.5492155558533138\n",
      "ave_R=0.5641031940778096\n",
      "ave_F1=0.5819691781486783\n",
      "ave_P=0.5794263716254916\n",
      "ave_R=0.5876447636456716\n",
      "ave_F1=0.5550072273382773\n",
      "ave_P=0.5471240683243825\n",
      "ave_R=0.573131901713518\n",
      "ave_F1=0.538495293416475\n",
      "ave_P=0.553263258934021\n",
      "ave_R=0.53483270877286\n",
      "ave_F1=0.5684222290913264\n",
      "ave_P=0.6049050688743591\n",
      "ave_R=0.5505863800644875\n",
      "ave_F1=0.5442057040176893\n",
      "ave_P=0.5825714335629815\n",
      "ave_R=0.5204274607332129\n",
      "ave_F1=0.5357175593192761\n",
      "ave_P=0.5089423807767721\n",
      "ave_R=0.5695971378913293\n",
      "ave_F1=0.5642634178511798\n",
      "ave_P=0.5875295009464025\n",
      "ave_R=0.5461089969612658\n",
      "ave_F1=0.5684185200929641\n",
      "ave_P=0.559713825583458\n",
      "ave_R=0.5963313615322113\n",
      "ave_F1=0.5777100589540269\n",
      "ave_P=0.5926292439301809\n",
      "ave_R=0.5658004780610403\n",
      "ave_F1=0.5774505744377773\n",
      "ave_P=0.5923362265030543\n",
      "ave_R=0.5677475288510323\n",
      "ave_F1=0.5501102829786172\n",
      "ave_P=0.5839879718938268\n",
      "ave_R=0.532342696324327\n",
      "ave_F1=0.5341684471445474\n",
      "ave_P=0.5711215373716856\n",
      "ave_R=0.5138509500096415\n",
      "ave_F1=0.5520383436232805\n",
      "ave_P=0.4922223500907421\n",
      "ave_R=0.6316455509513617\n",
      "ave_F1=0.5679413571077234\n",
      "ave_P=0.5931956408654943\n",
      "ave_R=0.556752332869698\n",
      "ave_F1=0.5836249101493094\n",
      "ave_P=0.5712586641311646\n",
      "ave_R=0.600666344165802\n",
      "ave_F1=0.5597271641095479\n",
      "ave_P=0.5704451832506392\n",
      "ave_R=0.5526275369856093\n",
      "ave_F1=0.5556661877781153\n",
      "ave_P=0.5568162398412824\n",
      "ave_R=0.561829375103116\n",
      "ave_F1=0.5860291302204133\n",
      "ave_P=0.5777241323675428\n",
      "ave_R=0.6013719137225833\n",
      "ave_F1=0.5781333887806306\n",
      "ave_P=0.5447756378696516\n",
      "ave_R=0.6259460460681182\n",
      "ave_F1=0.5369147381612233\n",
      "ave_P=0.5459139538662774\n",
      "ave_R=0.5289912628276008\n",
      "ave_F1=0.5909724039956927\n",
      "ave_P=0.584308291785419\n",
      "ave_R=0.6018872382119298\n",
      "ave_F1=0.5446872446272109\n",
      "ave_P=0.5977216534040592\n",
      "ave_R=0.5128235690019749\n",
      "ave_F1=0.5461179933764718\n",
      "ave_P=0.5263308535922657\n",
      "ave_R=0.5747943943197077\n",
      "ave_F1=0.5438512252909796\n",
      "ave_P=0.5366907387971878\n",
      "ave_R=0.5573911649840219\n",
      "ave_F1=0.5582000722487768\n",
      "ave_P=0.5532517363627751\n",
      "ave_R=0.5668120880921682\n",
      "ave_F1=0.5419542727371057\n",
      "ave_P=0.545749751230081\n",
      "ave_R=0.5434894822537899\n",
      "ave_F1=0.5854749818642934\n",
      "ave_P=0.5846695909897487\n",
      "ave_R=0.5891797572374344\n",
      "ave_F1=0.5801034376901739\n",
      "ave_P=0.593473811798236\n",
      "ave_R=0.5696733431781039\n",
      "ave_F1=0.6088035931954017\n",
      "ave_P=0.5649585196605096\n",
      "ave_R=0.6683880595060495\n",
      "ave_F1=0.5474366170388681\n",
      "ave_P=0.5961476729975806\n",
      "ave_R=0.5146888579115455\n",
      "ave_F1=0.5502298050201856\n",
      "ave_P=0.5350056522167645\n",
      "ave_R=0.5685294043559295\n",
      "ave_F1=0.6091760269620202\n",
      "ave_P=0.594618892940608\n",
      "ave_R=0.628176671537486\n",
      "ave_F1=0.5772421869966718\n",
      "ave_P=0.5761916100978851\n",
      "ave_R=0.5810133702225155\n",
      "ave_F1=0.5190752379130572\n",
      "ave_P=0.5641306913457811\n",
      "ave_R=0.494368297746405\n",
      "ave_F1=0.5445569600496027\n",
      "ave_P=0.565503274401029\n",
      "ave_R=0.5396455327669779\n",
      "ave_F1=0.5656687350322803\n",
      "ave_P=0.5861415943751732\n",
      "ave_R=0.5581694835176071\n",
      "ave_F1=0.5296588654319445\n",
      "ave_P=0.5636512676874796\n",
      "ave_R=0.5140164951483409\n",
      "ave_F1=0.5751569103449583\n",
      "ave_P=0.5663856817409396\n",
      "ave_R=0.5886775879189372\n",
      "ave_F1=0.5410779918943133\n",
      "ave_P=0.5431371778249741\n",
      "ave_R=0.5436038877282824\n",
      "ave_F1=0.4986368667196344\n",
      "ave_P=0.5231710802625726\n",
      "ave_R=0.49090320180963587\n",
      "ave_F1=0.5194207831765666\n",
      "ave_P=0.5852602079059138\n",
      "ave_R=0.47695158557458356\n",
      "ave_F1=0.5475344897485247\n",
      "ave_P=0.5599555443314945\n",
      "ave_R=0.5395920989560146\n",
      "ave_F1=0.5531647205352783\n",
      "ave_P=0.5887975181852069\n",
      "ave_R=0.5237764875803675\n",
      "ave_F1=0.540463722365744\n",
      "ave_P=0.573054321548518\n",
      "ave_R=0.5224932839765268\n",
      "ave_F1=0.5456367901393345\n",
      "ave_P=0.5488946813912619\n",
      "ave_R=0.5450302596603122\n",
      "ave_F1=0.5216728838590475\n",
      "ave_P=0.5180782962303895\n",
      "ave_R=0.5281710532995371\n",
      "ave_F1=0.5521561529325403\n",
      "ave_P=0.5513171065544736\n",
      "ave_R=0.5692795877871306\n",
      "ave_F1=0.5394947620538565\n",
      "ave_P=0.5456270175012528\n",
      "ave_R=0.5373019163425152\n",
      "ave_F1=0.5042640667695265\n",
      "ave_P=0.5201407148287847\n",
      "ave_R=0.5013645671881162\n",
      "ave_F1=0.610339734951655\n",
      "ave_P=0.613619863986969\n",
      "ave_R=0.6160865565141042\n",
      "ave_F1=0.5454677092177528\n",
      "ave_P=0.5416773659842354\n",
      "ave_R=0.5511455523116248\n",
      "ave_F1=0.4903876365177215\n",
      "ave_P=0.4871541110296098\n",
      "ave_R=0.4951624700001308\n",
      "ave_F1=0.5352270170473136\n",
      "ave_P=0.5742504020723013\n",
      "ave_R=0.5156040102816545\n",
      "ave_F1=0.5701296017505229\n",
      "ave_P=0.5808814279735088\n",
      "ave_R=0.5640187100507319\n",
      "ave_F1=0.5484231236789908\n",
      "ave_P=0.5554831464375768\n",
      "ave_R=0.5463604895131928\n",
      "ave_F1=0.5298320638636748\n",
      "ave_P=0.5326882414519787\n",
      "ave_R=0.5290896209577719\n",
      "ave_F1=0.53324513806813\n",
      "ave_P=0.5663259469944498\n",
      "ave_R=0.514825733675473\n",
      "ave_F1=0.5274671074002981\n",
      "ave_P=0.5604062946513295\n",
      "ave_R=0.5002862038090825\n",
      "ave_F1=0.5744374175038602\n",
      "ave_P=0.5712852738797665\n",
      "ave_R=0.5804466667274634\n",
      "ave_F1=0.5722699463367462\n",
      "ave_P=0.5554511802537101\n",
      "ave_R=0.5970150459380377\n",
      "ave_F1=0.5392717866283474\n",
      "ave_P=0.5463941192085092\n",
      "ave_R=0.5569717557141275\n",
      "ave_F1=0.5515365073723453\n",
      "ave_P=0.5741187467106751\n",
      "ave_R=0.5354264064558915\n",
      "ave_F1=0.6182719854747548\n",
      "ave_P=0.6184663053821114\n",
      "ave_R=0.6282307190053603\n",
      "ave_F1=0.5582546850755101\n",
      "ave_P=0.5735635846143677\n",
      "ave_R=0.5464559467065901\n",
      "ave_F1=0.5590264841742899\n",
      "ave_P=0.5799210242841436\n",
      "ave_R=0.5521497322225023\n",
      "ave_F1=0.5767875770106912\n",
      "ave_P=0.5748000930373868\n",
      "ave_R=0.5819003712385893\n",
      "ave_F1=0.5454716647372526\n",
      "ave_P=0.5246048315482981\n",
      "ave_R=0.5732251090161941\n",
      "ave_F1=0.5432171141892149\n",
      "ave_P=0.5833991028760609\n",
      "ave_R=0.5177115348347446\n",
      "ave_F1=0.5351291278546507\n",
      "ave_P=0.5869289426641031\n",
      "ave_R=0.5068812370300293\n",
      "ave_F1=0.555477570023453\n",
      "ave_P=0.532644569350962\n",
      "ave_R=0.5853746716390577\n",
      "ave_F1=0.5814319638644948\n",
      "ave_P=0.5694807434783262\n",
      "ave_R=0.5998124638024498\n",
      "ave_F1=0.5438347072513016\n",
      "ave_P=0.5505892071459029\n",
      "ave_R=0.5403672683018225\n",
      "ave_F1=0.5795231249597338\n",
      "ave_P=0.5833985447883606\n",
      "ave_R=0.5800940573215485\n",
      "ave_F1=0.5732875299453736\n",
      "ave_P=0.5959240317344665\n",
      "ave_R=0.5667850291728973\n",
      "ave_F1=0.5443500829013911\n",
      "ave_P=0.5936486094512723\n",
      "ave_R=0.513009004633535\n",
      "ave_F1=0.5525861954689026\n",
      "ave_P=0.5972231739759445\n",
      "ave_R=0.5262624400854111\n",
      "ave_F1=0.5511333109544856\n",
      "ave_P=0.5427782932030303\n",
      "ave_R=0.5632459257862398\n",
      "ave_F1=0.5635822263726017\n",
      "ave_P=0.6056054689382252\n",
      "ave_R=0.5328319773339388\n",
      "ave_F1=0.5496466560289264\n",
      "ave_P=0.543779025785625\n",
      "ave_R=0.5589346420019865\n",
      "ave_F1=0.5584454242140054\n",
      "ave_P=0.57788077108562\n",
      "ave_R=0.5430607624351979\n",
      "ave_F1=0.5644994162415203\n",
      "ave_P=0.5674247549552667\n",
      "ave_R=0.5637997098659214\n",
      "ave_F1=0.527607853213946\n",
      "ave_P=0.5741809799715325\n",
      "ave_R=0.49642112409627\n",
      "ave_F1=0.5209018255655582\n",
      "ave_P=0.5388476630816093\n",
      "ave_R=0.5065220261995609\n",
      "ave_F1=0.5335689676113617\n",
      "ave_P=0.5325311047908587\n",
      "ave_R=0.5382977112745627\n",
      "ave_F1=0.5354688207308451\n",
      "ave_P=0.5362405200799306\n",
      "ave_R=0.5374626119931539\n",
      "ave_F1=0.5319843943913778\n",
      "ave_P=0.5699104408423106\n",
      "ave_R=0.510368006626765\n",
      "ave_F1=0.5679108632935418\n",
      "ave_P=0.5595790849791633\n",
      "ave_R=0.5785435514317618\n",
      "ave_F1=0.5876880364887642\n",
      "ave_P=0.5905526384259715\n",
      "ave_R=0.5953710946169767\n",
      "ave_F1=0.5474774025380611\n",
      "ave_P=0.5827405352145434\n",
      "ave_R=0.5273589722812175\n",
      "ave_F1=0.5226003600491418\n",
      "ave_P=0.4866003167629242\n",
      "ave_R=0.5816277937094371\n",
      "ave_F1=0.5515029529730479\n",
      "ave_P=0.5899330866005685\n",
      "ave_R=0.5326520688831806\n",
      "ave_F1=0.5405797809362411\n",
      "ave_P=0.5384752523899078\n",
      "ave_R=0.5477947527170182\n",
      "ave_F1=0.5510458487730759\n",
      "ave_P=0.5808452321932867\n",
      "ave_R=0.5247462185529562\n",
      "ave_F1=0.5551569875743654\n",
      "ave_P=0.544220099846522\n",
      "ave_R=0.568648969133695\n",
      "ave_F1=0.578861626131194\n",
      "ave_P=0.6277056208678654\n",
      "ave_R=0.5447579068796975\n",
      "ave_F1=0.5693572349306466\n",
      "ave_P=0.5958598014237224\n",
      "ave_R=0.5554454814696658\n",
      "ave_F1=0.5500046390419205\n",
      "ave_P=0.5924944973861178\n",
      "ave_R=0.5284481110672156\n",
      "ave_F1=0.5783502438612151\n",
      "ave_P=0.5850261052449545\n",
      "ave_R=0.5832076464828692\n",
      "ave_F1=0.5616881017174039\n",
      "ave_P=0.5500831534819943\n",
      "ave_R=0.5752820393868855\n",
      "ave_F1=0.5599854345505054\n",
      "ave_P=0.549848032456178\n",
      "ave_R=0.5719955769868997\n",
      "ave_F1=0.5641572062785809\n",
      "ave_P=0.583118920142834\n",
      "ave_R=0.5477386116981506\n",
      "ave_F1=0.5768118133147557\n",
      "ave_P=0.6051113645235697\n",
      "ave_R=0.5529761324326198\n",
      "ave_F1=0.5596267647649112\n",
      "ave_P=0.568282569709577\n",
      "ave_R=0.5550925366972622\n",
      "ave_F1=0.548144654267364\n",
      "ave_P=0.5400017930401696\n",
      "ave_R=0.5605443194508553\n",
      "ave_F1=0.5932784062443357\n",
      "ave_P=0.6007469928625858\n",
      "ave_R=0.5880493395256273\n",
      "ave_F1=0.5437228947877883\n",
      "ave_P=0.539087146272262\n",
      "ave_R=0.5525743400057157\n",
      "ave_F1=0.5524663906066846\n",
      "ave_P=0.5906274184966699\n",
      "ave_R=0.5328380308854275\n",
      "ave_F1=0.5468087908767518\n",
      "ave_P=0.552675157501584\n",
      "ave_R=0.5541538774967194\n",
      "ave_F1=0.570913293344133\n",
      "ave_P=0.5805056971662185\n",
      "ave_R=0.5642029476516387\n",
      "ave_F1=0.5597607927364215\n",
      "ave_P=0.5708316721414265\n",
      "ave_R=0.5658218133867833\n",
      "ave_F1=0.5223825726037225\n",
      "ave_P=0.5550927513589462\n",
      "ave_R=0.5028078090399504\n",
      "ave_F1=0.5902351631837732\n",
      "ave_P=0.5927401377874262\n",
      "ave_R=0.5894212648272514\n",
      "ave_F1=0.5648913224538167\n",
      "ave_P=0.5803610424200694\n",
      "ave_R=0.562271922826767\n",
      "ave_F1=0.5484070892919574\n",
      "ave_P=0.591744897135517\n",
      "ave_R=0.5246525438208329\n",
      "ave_F1=0.5659228158183396\n",
      "ave_P=0.5754405278712511\n",
      "ave_R=0.5589168551377952\n",
      "ave_F1=0.564061712887552\n",
      "ave_P=0.5915195292068852\n",
      "ave_R=0.5500002238485548\n",
      "ave_F1=0.5526083353687735\n",
      "ave_P=0.5462680946378147\n",
      "ave_R=0.5622832436771954\n",
      "ave_F1=0.5827633192141851\n",
      "ave_P=0.6093173543612163\n",
      "ave_R=0.5620802372694016\n",
      "ave_F1=0.54451884329319\n",
      "ave_P=0.5266351078947386\n",
      "ave_R=0.5691356937090556\n",
      "ave_F1=0.5437532881540912\n",
      "ave_P=0.5338745082595518\n",
      "ave_R=0.5604556410440377\n",
      "ave_F1=0.5854054067064735\n",
      "ave_P=0.6054330231512294\n",
      "ave_R=0.5702995324836058\n",
      "ave_F1=0.583245250582695\n",
      "ave_P=0.5514829248189926\n",
      "ave_R=0.6212608575820923\n",
      "ave_F1=0.5560491120273416\n",
      "ave_P=0.586179259148511\n",
      "ave_R=0.5428096169775183\n",
      "ave_F1=0.5584792200475931\n",
      "ave_P=0.5531899761408567\n",
      "ave_R=0.5729761313647032\n",
      "ave_F1=0.52852828282377\n",
      "ave_P=0.5996951570977336\n",
      "ave_R=0.4836245665083761\n",
      "ave_F1=0.5967923531929652\n",
      "ave_P=0.6057452470064163\n",
      "ave_R=0.5925394376118978\n",
      "ave_F1=0.5691606764766303\n",
      "ave_P=0.577822161669081\n",
      "ave_R=0.5711745623160492\n",
      "ave_F1=0.5979506630450487\n",
      "ave_P=0.5938340421766043\n",
      "ave_R=0.6055359292775393\n",
      "ave_F1=0.5933595725468227\n",
      "ave_P=0.587034081419309\n",
      "ave_R=0.6025756845871607\n",
      "ave_F1=0.5226745949614615\n",
      "ave_P=0.5720186354148955\n",
      "ave_R=0.4966551686326663\n",
      "ave_F1=0.5527394582395968\n",
      "ave_P=0.5582628893679467\n",
      "ave_R=0.5641786356767019\n",
      "ave_F1=0.555241893529892\n",
      "ave_P=0.5956223845481873\n",
      "ave_R=0.531718552907308\n",
      "ave_F1=0.5636981863241929\n",
      "ave_P=0.5768434863824111\n",
      "ave_R=0.5530073195695877\n",
      "ave_F1=0.5077602631515927\n",
      "ave_P=0.5343368161055777\n",
      "ave_R=0.49321354428927106\n",
      "ave_F1=0.6104392949491739\n",
      "ave_P=0.580382727086544\n",
      "ave_R=0.647648511454463\n",
      "ave_F1=0.5757714406802104\n",
      "ave_P=0.5910247037043939\n",
      "ave_R=0.5636123969004705\n",
      "ave_F1=0.5491025671362877\n",
      "ave_P=0.5628725094454629\n",
      "ave_R=0.5468137147171157\n",
      "ave_F1=0.5704947526638324\n",
      "ave_P=0.6131298733063233\n",
      "ave_R=0.5450328783347056\n",
      "ave_F1=0.5978876643710667\n",
      "ave_P=0.5764350036780039\n",
      "ave_R=0.6244263867537181\n",
      "ave_F1=0.44794454239308834\n",
      "ave_P=0.4468159219250083\n",
      "ave_R=0.4568283408880234\n",
      "ave_F1=0.5858663921909673\n",
      "ave_P=0.5920963776963097\n",
      "ave_R=0.5820824200553554\n",
      "ave_F1=0.5561338114951339\n",
      "ave_P=0.5733948578791959\n",
      "ave_R=0.5480595575202079\n",
      "ave_F1=0.5754904970526695\n",
      "ave_P=0.5632645333451884\n",
      "ave_R=0.5924252287617752\n",
      "ave_F1=0.5765514236230117\n",
      "ave_P=0.5412793128918378\n",
      "ave_R=0.6240500868895115\n",
      "ave_F1=0.5657237575466472\n",
      "ave_P=0.5697018853703836\n",
      "ave_R=0.5676363825349879\n",
      "ave_F1=0.5373508738619941\n",
      "ave_P=0.5665149348122733\n",
      "ave_R=0.5155899560167676\n",
      "ave_F1=0.5523032622459607\n",
      "ave_P=0.5883153627316157\n",
      "ave_R=0.5320532004802655\n",
      "ave_F1=0.5437332350833743\n",
      "ave_P=0.54560086832327\n",
      "ave_R=0.5452440655698964\n",
      "ave_F1=0.5602484803933364\n",
      "ave_P=0.5422046406146808\n",
      "ave_R=0.5890868680599408\n",
      "ave_F1=0.5609806196557151\n",
      "ave_P=0.5649143730600675\n",
      "ave_R=0.5629977169964049\n",
      "ave_F1=0.5554554556374964\n",
      "ave_P=0.5577439665794373\n",
      "ave_R=0.5651284028654513\n",
      "ave_F1=0.5714729848233137\n",
      "ave_P=0.56838539310477\n",
      "ave_R=0.575690047307448\n",
      "ave_F1=0.5356760091251798\n",
      "ave_P=0.5571092817518446\n",
      "ave_R=0.5287629856003655\n",
      "ave_F1=0.5461011976003647\n",
      "ave_P=0.5903054192662239\n",
      "ave_R=0.5172092542052269\n",
      "ave_F1=0.5513346542914709\n",
      "ave_P=0.564921673387289\n",
      "ave_R=0.5397569487492243\n",
      "ave_F1=0.5673951723358848\n",
      "ave_P=0.5623114962469448\n",
      "ave_R=0.576338682662357\n",
      "ave_F1=0.5309168118983507\n",
      "ave_P=0.5338273998349905\n",
      "ave_R=0.528905151411891\n",
      "ave_F1=0.5617670249193907\n",
      "ave_P=0.5679896841757\n",
      "ave_R=0.5571490847505629\n",
      "ave_F1=0.5160448319382138\n",
      "ave_P=0.5876440902551016\n",
      "ave_R=0.469423340426551\n",
      "ave_F1=0.5533948509316695\n",
      "ave_P=0.5612318872621185\n",
      "ave_R=0.5490962415933609\n",
      "ave_F1=0.5245691279278082\n",
      "ave_P=0.5591674565392382\n",
      "ave_R=0.5058296222020598\n",
      "ave_F1=0.5674565338662693\n",
      "ave_P=0.5365138415779386\n",
      "ave_R=0.6108819235648427\n",
      "ave_F1=0.5122315724906714\n",
      "ave_P=0.5391944611202115\n",
      "ave_R=0.502337797180466\n",
      "ave_F1=0.5430214464664459\n",
      "ave_P=0.5976535886526108\n",
      "ave_R=0.5071254953742027\n",
      "ave_F1=0.5855573632215199\n",
      "ave_P=0.5802871424900858\n",
      "ave_R=0.5956289435687818\n",
      "ave_F1=0.5542165028552214\n",
      "ave_P=0.5642107309152683\n",
      "ave_R=0.5469585979978243\n",
      "ave_F1=0.530693015226951\n",
      "ave_P=0.5931686323422652\n",
      "ave_R=0.4927758150375806\n",
      "ave_F1=0.5521869709094366\n",
      "ave_P=0.5685241341590881\n",
      "ave_R=0.5489733432730038\n",
      "ave_F1=0.5545612514019013\n",
      "ave_P=0.5388038948178291\n",
      "ave_R=0.5738655462861061\n",
      "ave_F1=0.5472558625042439\n",
      "ave_P=0.547564764196674\n",
      "ave_R=0.5503229921062788\n",
      "ave_F1=0.609831419061212\n",
      "ave_P=0.5917550640947679\n",
      "ave_R=0.6312583316774929\n",
      "ave_F1=0.5250191716477275\n",
      "ave_P=0.5434047402814031\n",
      "ave_R=0.5124385085267326\n",
      "ave_F1=0.5524758054150476\n",
      "ave_P=0.5414630158079995\n",
      "ave_R=0.5714411122931374\n",
      "ave_F1=0.572092684606711\n",
      "ave_P=0.5797550485779842\n",
      "ave_R=0.5674660031994184\n",
      "ave_F1=0.5450821710355354\n",
      "ave_P=0.5415907968174327\n",
      "ave_R=0.5549798692717697\n",
      "ave_F1=0.5332223010063172\n",
      "ave_P=0.5623174126148224\n",
      "ave_R=0.5201303224563598\n",
      "ave_F1=0.512716852956348\n",
      "ave_P=0.5166023890177409\n",
      "ave_R=0.5118475986851586\n",
      "ave_F1=0.576127503067255\n",
      "ave_P=0.5492419693619013\n",
      "ave_R=0.6080758832395077\n",
      "ave_F1=0.5344048911525358\n",
      "ave_P=0.5767992456113139\n",
      "ave_R=0.5104006118671869\n",
      "ave_F1=0.525726427252476\n",
      "ave_P=0.5159073804433529\n",
      "ave_R=0.538444737975414\n",
      "ave_F1=0.5560502306773112\n",
      "ave_P=0.5489982939683474\n",
      "ave_R=0.5671048221679834\n",
      "ave_F1=0.567234317804205\n",
      "ave_P=0.5814023500886457\n",
      "ave_R=0.5650545420317814\n",
      "ave_F1=0.5282561913132667\n",
      "ave_P=0.48163063228130343\n",
      "ave_R=0.6009212404489517\n",
      "ave_F1=0.529973116889596\n",
      "ave_P=0.534024727717042\n",
      "ave_R=0.5392267368733883\n",
      "ave_F1=0.55896531889836\n",
      "ave_P=0.5891772627830505\n",
      "ave_R=0.5335747535030048\n",
      "ave_F1=0.5827105028761758\n",
      "ave_P=0.568183432298678\n",
      "ave_R=0.6027915985495956\n",
      "ave_F1=0.5978191362486945\n",
      "ave_P=0.5918415069580079\n",
      "ave_R=0.6063431481520335\n",
      "ave_F1=0.5614304516523604\n",
      "ave_P=0.5500155971163795\n",
      "ave_R=0.5788734498478117\n",
      "ave_F1=0.5272438829853421\n",
      "ave_P=0.5595165596121834\n",
      "ave_R=0.5113583868458158\n",
      "ave_F1=0.5100820707873657\n",
      "ave_P=0.5666642203353919\n",
      "ave_R=0.4802850719827872\n",
      "ave_F1=0.5123749560478962\n",
      "ave_P=0.5592642132983063\n",
      "ave_R=0.48814001047249994\n",
      "ave_F1=0.5594896326462427\n",
      "ave_P=0.5507112959468806\n",
      "ave_R=0.5730096192823516\n",
      "ave_F1=0.5634461633670025\n",
      "ave_P=0.5806673948581402\n",
      "ave_R=0.5508177112310361\n",
      "ave_F1=0.5628737527877092\n",
      "ave_P=0.5980841089040041\n",
      "ave_R=0.5416420167312026\n",
      "ave_F1=0.5170170565446218\n",
      "ave_P=0.5804748570635205\n",
      "ave_R=0.48031718957991826\n",
      "ave_F1=0.5588625236636117\n",
      "ave_P=0.5466510586085773\n",
      "ave_R=0.5925963421662649\n",
      "ave_F1=0.539212787989527\n",
      "ave_P=0.532390670850873\n",
      "ave_R=0.5482221492566168\n",
      "ave_F1=0.5439302008599043\n",
      "ave_P=0.5652798973023891\n",
      "ave_R=0.5326018270105124\n",
      "ave_F1=0.5428910514011103\n",
      "ave_P=0.5363743638291079\n",
      "ave_R=0.5510964696021641\n",
      "ave_F1=0.5372024327516556\n",
      "ave_P=0.528671104993139\n",
      "ave_R=0.5480118138449532\n",
      "ave_F1=0.5962378435664707\n",
      "ave_P=0.582795712682936\n",
      "ave_R=0.6133786532613966\n",
      "ave_F1=0.5505156133856092\n",
      "ave_P=0.5957160294055939\n",
      "ave_R=0.5140149295330048\n",
      "ave_F1=0.5650226434071859\n",
      "ave_P=0.5532648106416066\n",
      "ave_R=0.5788766827848223\n",
      "ave_F1=0.6031166967891511\n",
      "ave_P=0.6264102217696962\n",
      "ave_R=0.5919155194645837\n",
      "ave_F1=0.575019289497976\n",
      "ave_P=0.5667234504664386\n",
      "ave_R=0.588736214019634\n",
      "ave_F1=0.508392421901226\n",
      "ave_P=0.49301958133776985\n",
      "ave_R=0.5342345928152402\n",
      "ave_F1=0.5861960084814775\n",
      "ave_P=0.5746131735412698\n",
      "ave_R=0.5995924127729315\n",
      "ave_F1=0.5235403227705916\n",
      "ave_P=0.5228698438956958\n",
      "ave_R=0.5293018537409165\n",
      "ave_F1=0.5294202160504129\n",
      "ave_P=0.5224809389975336\n",
      "ave_R=0.541255519621902\n",
      "ave_F1=0.5573635995388031\n",
      "ave_P=0.5607812470859952\n",
      "ave_R=0.5556932522190942\n",
      "ave_F1=0.5941563874483109\n",
      "ave_P=0.5797049963474273\n",
      "ave_R=0.6132474011182785\n",
      "ave_F1=0.598865376578437\n",
      "ave_P=0.5906986408763462\n",
      "ave_R=0.6095861130290561\n",
      "ave_F1=0.5594163729864008\n",
      "ave_P=0.5513188120196847\n",
      "ave_R=0.5716435839148128\n",
      "ave_F1=0.5837864024298531\n",
      "ave_P=0.5683683965887342\n",
      "ave_R=0.6024922956617511\n",
      "ave_F1=0.5727198653750949\n",
      "ave_P=0.5455437435044183\n",
      "ave_R=0.6059327169700905\n",
      "ave_F1=0.5514719374974568\n",
      "ave_P=0.547526474793752\n",
      "ave_R=0.5591535460948944\n",
      "ave_F1=0.5317778517218197\n",
      "ave_P=0.5317888952353421\n",
      "ave_R=0.534566639977343\n",
      "ave_F1=0.5751734860241413\n",
      "ave_P=0.577878812327981\n",
      "ave_R=0.57543461676687\n",
      "ave_F1=0.5436292515860663\n",
      "ave_P=0.5398576100667317\n",
      "ave_R=0.5547395613458421\n",
      "ave_F1=0.5599159324169158\n",
      "ave_P=0.5580615478754044\n",
      "ave_R=0.5682439172267914\n",
      "ave_F1=0.5237323753535748\n",
      "ave_P=0.5322919276853403\n",
      "ave_R=0.5166241526603699\n",
      "ave_F1=0.5561900426944096\n",
      "ave_P=0.5527380347251892\n",
      "ave_R=0.5633620858192444\n",
      "ave_F1=0.5706226819870519\n",
      "ave_P=0.5688550998182857\n",
      "ave_R=0.5748686807997087\n",
      "ave_F1=0.5061120380248342\n",
      "ave_P=0.5216793471149036\n",
      "ave_R=0.4958292299083301\n",
      "ave_F1=0.5657549457890647\n",
      "ave_P=0.5865350493362972\n",
      "ave_R=0.5496942571231297\n",
      "ave_F1=0.5543475074072679\n",
      "ave_P=0.5750681395332019\n",
      "ave_R=0.5543579695125421\n",
      "ave_F1=0.515039100976927\n",
      "ave_P=0.527007231754916\n",
      "ave_R=0.5064236298203468\n",
      "ave_F1=0.5249856272860179\n",
      "ave_P=0.5478305804824072\n",
      "ave_R=0.5072540070802446\n",
      "ave_F1=0.5642758475785906\n",
      "ave_P=0.5652346489104357\n",
      "ave_R=0.5672941204499115\n",
      "ave_F1=0.5397821652392546\n",
      "ave_P=0.5248707247277101\n",
      "ave_R=0.5573161145051321\n",
      "ave_F1=0.5887690981229147\n",
      "ave_P=0.6137401627169715\n",
      "ave_R=0.5698358275824122\n",
      "ave_F1=0.5884594952358919\n",
      "ave_P=0.5992395229199353\n",
      "ave_R=0.5823047090979183\n",
      "ave_F1=0.5543846823275089\n",
      "ave_P=0.5650161509402096\n",
      "ave_R=0.5473280679434538\n",
      "ave_F1=0.5653225017281679\n",
      "ave_P=0.5933588760403486\n",
      "ave_R=0.5527358037921098\n",
      "ave_F1=0.5268355491368667\n",
      "ave_P=0.5584500496802123\n",
      "ave_R=0.5056553508924402\n",
      "ave_F1=0.5962654100014613\n",
      "ave_P=0.5740267634391785\n",
      "ave_R=0.6216556131839752\n",
      "ave_F1=0.5610825530120305\n",
      "ave_P=0.5657290135111127\n",
      "ave_R=0.5691426983901433\n",
      "ave_F1=0.5337394212975222\n",
      "ave_P=0.5983509745667962\n",
      "ave_R=0.49005554441143484\n",
      "ave_F1=0.5375799114289491\n",
      "ave_P=0.5756337653035702\n",
      "ave_R=0.5168530650760816\n",
      "ave_F1=0.537441414026987\n",
      "ave_P=0.5731468519994191\n",
      "ave_R=0.5091338938190824\n",
      "ave_F1=0.5792549805364747\n",
      "ave_P=0.6051599154437798\n",
      "ave_R=0.5695111596066019\n",
      "ave_F1=0.5495208081077126\n",
      "ave_P=0.5486136184019201\n",
      "ave_R=0.5518906887839822\n",
      "ave_F1=0.5639211396376292\n",
      "ave_P=0.5370712955792745\n",
      "ave_R=0.5993245214223861\n",
      "ave_F1=0.5458721330872288\n",
      "ave_P=0.581474765031426\n",
      "ave_R=0.5295664430768402\n",
      "ave_F1=0.5733934052680668\n",
      "ave_P=0.6001750301373633\n",
      "ave_R=0.5570847556779259\n",
      "ave_F1=0.5451708010264805\n",
      "ave_P=0.5019071982020423\n",
      "ave_R=0.6145341935611907\n",
      "ave_F1=0.5485518480602064\n",
      "ave_P=0.5496849381087119\n",
      "ave_R=0.5491316313283485\n",
      "ave_F1=0.6042201691203647\n",
      "ave_P=0.5796810408433278\n",
      "ave_R=0.6339327600267198\n",
      "ave_F1=0.5642191465747984\n",
      "ave_P=0.5964533059220565\n",
      "ave_R=0.5486435905883187\n",
      "ave_F1=0.566072112818559\n",
      "ave_P=0.5828758185108502\n",
      "ave_R=0.5594711909691493\n",
      "ave_F1=0.565699308684894\n",
      "ave_P=0.604554338327476\n",
      "ave_R=0.5442642467362541\n"
     ]
    }
   ],
   "source": [
    "all_res_L = []\n",
    "for i in range(len(response_filtered_reqL)) :\n",
    "    refs = sent_tokenize(feedback_L[i])\n",
    "    cands = sent_tokenize(response_filtered_reqL[i])\n",
    "    res_L = get_pairwise_BertScore(refs, cands) # the res_L is formatted as the order of F1, P, R\n",
    "    all_res_L.append(res_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_DF = pd.DataFrame(all_res_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5548167836937625"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res_DF[0].sum()/len(all_res_DF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5653681313313538"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res_DF[1].sum()/len(all_res_DF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5530824037859886"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res_DF[2].sum()/len(all_res_DF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_res_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(hypothesis, reference):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Rouge_res_L = []\n",
    "for i in range(235) :\n",
    "    scores = calculate_rouge(response_filtered_reqL[i], feedback_L[i])\n",
    "    all_Rouge_res_L.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_sum = 0\n",
    "r2_sum = 0\n",
    "rl_sum = 0\n",
    "for i in all_Rouge_res_L :\n",
    "    r1_sum = i['rouge-1']['f'] + r1_sum\n",
    "    r2_sum = i['rouge-2']['f'] + r2_sum\n",
    "    rl_sum = i['rouge-l']['f'] + rl_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13605079935164094"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1_sum/len(all_Rouge_res_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014119603813034107"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_sum/len(all_Rouge_res_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12621973581778176"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_sum/len(all_Rouge_res_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E2011</td>\n",
       "      <td>This page provides a description of the Expert...</td>\n",
       "      <td>The wiki was to the point and explains all the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E1682</td>\n",
       "      <td>Expertiza is an open source project for school...</td>\n",
       "      <td>The doc could be significantly improved by pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E1628</td>\n",
       "      <td>The main goal of the project is to present the...</td>\n",
       "      <td>The score should be x out of y, or \"x/y\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E1758</td>\n",
       "      <td>Expertiza is a web application developed using...</td>\n",
       "      <td>Design doc is quite readable.  Would have help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E2016</td>\n",
       "      <td>In the first round of the Expertiza reviews, r...</td>\n",
       "      <td>The document describes the project well in nar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>E2107</td>\n",
       "      <td>Expertiza has a functionality that allows stud...</td>\n",
       "      <td>The page is pretty well organized in what chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>E1649</td>\n",
       "      <td>Expertiza is an Open Source Software project d...</td>\n",
       "      <td>Very good job of describing the requirements a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>E2070</td>\n",
       "      <td>A response is the object that is created when ...</td>\n",
       "      <td>There are good descriptions of the code change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>E2022</td>\n",
       "      <td>&lt;link&gt; provides a feature to peer review other...</td>\n",
       "      <td>The prose is quite readable.  The sequence dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>E2077</td>\n",
       "      <td>Currently, Expertiza has no way to associate m...</td>\n",
       "      <td>The changes are comprehensively described.  Ho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pid                                           document  \\\n",
       "0    E2011  This page provides a description of the Expert...   \n",
       "1    E1682  Expertiza is an open source project for school...   \n",
       "2    E1628  The main goal of the project is to present the...   \n",
       "3    E1758  Expertiza is a web application developed using...   \n",
       "4    E2016  In the first round of the Expertiza reviews, r...   \n",
       "..     ...                                                ...   \n",
       "476  E2107  Expertiza has a functionality that allows stud...   \n",
       "477  E1649  Expertiza is an Open Source Software project d...   \n",
       "479  E2070  A response is the object that is created when ...   \n",
       "482  E2022  <link> provides a feature to peer review other...   \n",
       "483  E2077  Currently, Expertiza has no way to associate m...   \n",
       "\n",
       "                                               summary  \n",
       "0    The wiki was to the point and explains all the...  \n",
       "1    The doc could be significantly improved by pro...  \n",
       "2            The score should be x out of y, or \"x/y\".  \n",
       "3    Design doc is quite readable.  Would have help...  \n",
       "4    The document describes the project well in nar...  \n",
       "..                                                 ...  \n",
       "476  The page is pretty well organized in what chan...  \n",
       "477  Very good job of describing the requirements a...  \n",
       "479  There are good descriptions of the code change...  \n",
       "482  The prose is quite readable.  The sequence dia...  \n",
       "483  The changes are comprehensively described.  Ho...  \n",
       "\n",
       "[427 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_filtered_reqL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['openai_generated'] = response_filtered_reqL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>openai_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E2011</td>\n",
       "      <td>This page provides a description of the Expert...</td>\n",
       "      <td>The wiki was to the point and explains all the...</td>\n",
       "      <td>You have done an excellent job explaining the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E1682</td>\n",
       "      <td>Expertiza is an open source project for school...</td>\n",
       "      <td>The doc could be significantly improved by pro...</td>\n",
       "      <td>This is a thorough and comprehensive assignmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E1628</td>\n",
       "      <td>The main goal of the project is to present the...</td>\n",
       "      <td>The score should be x out of y, or \"x/y\".</td>\n",
       "      <td>This is a comprehensive assignment, with clear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E1758</td>\n",
       "      <td>Expertiza is a web application developed using...</td>\n",
       "      <td>Design doc is quite readable.  Would have help...</td>\n",
       "      <td>Congratulations on the completion of your proj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E2016</td>\n",
       "      <td>In the first round of the Expertiza reviews, r...</td>\n",
       "      <td>The document describes the project well in nar...</td>\n",
       "      <td>This is a very detailed and thorough assignmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>E2107</td>\n",
       "      <td>Expertiza has a functionality that allows stud...</td>\n",
       "      <td>The page is pretty well organized in what chan...</td>\n",
       "      <td>Dear Student,\\n\\nThank you for submitting your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>E1649</td>\n",
       "      <td>Expertiza is an Open Source Software project d...</td>\n",
       "      <td>Very good job of describing the requirements a...</td>\n",
       "      <td>The assignment is well-written and includes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>E2070</td>\n",
       "      <td>A response is the object that is created when ...</td>\n",
       "      <td>There are good descriptions of the code change...</td>\n",
       "      <td>Thank you for your well-detailed approach to r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>E2022</td>\n",
       "      <td>&lt;link&gt; provides a feature to peer review other...</td>\n",
       "      <td>The prose is quite readable.  The sequence dia...</td>\n",
       "      <td>Thank you for your submission. The detailed un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>E2077</td>\n",
       "      <td>Currently, Expertiza has no way to associate m...</td>\n",
       "      <td>The changes are comprehensively described.  Ho...</td>\n",
       "      <td>Your assignment showcases an excellent underst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pid                                           document  \\\n",
       "0    E2011  This page provides a description of the Expert...   \n",
       "1    E1682  Expertiza is an open source project for school...   \n",
       "2    E1628  The main goal of the project is to present the...   \n",
       "3    E1758  Expertiza is a web application developed using...   \n",
       "4    E2016  In the first round of the Expertiza reviews, r...   \n",
       "..     ...                                                ...   \n",
       "476  E2107  Expertiza has a functionality that allows stud...   \n",
       "477  E1649  Expertiza is an Open Source Software project d...   \n",
       "479  E2070  A response is the object that is created when ...   \n",
       "482  E2022  <link> provides a feature to peer review other...   \n",
       "483  E2077  Currently, Expertiza has no way to associate m...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    The wiki was to the point and explains all the...   \n",
       "1    The doc could be significantly improved by pro...   \n",
       "2            The score should be x out of y, or \"x/y\".   \n",
       "3    Design doc is quite readable.  Would have help...   \n",
       "4    The document describes the project well in nar...   \n",
       "..                                                 ...   \n",
       "476  The page is pretty well organized in what chan...   \n",
       "477  Very good job of describing the requirements a...   \n",
       "479  There are good descriptions of the code change...   \n",
       "482  The prose is quite readable.  The sequence dia...   \n",
       "483  The changes are comprehensively described.  Ho...   \n",
       "\n",
       "                                      openai_generated  \n",
       "0    You have done an excellent job explaining the ...  \n",
       "1    This is a thorough and comprehensive assignmen...  \n",
       "2    This is a comprehensive assignment, with clear...  \n",
       "3    Congratulations on the completion of your proj...  \n",
       "4    This is a very detailed and thorough assignmen...  \n",
       "..                                                 ...  \n",
       "476  Dear Student,\\n\\nThank you for submitting your...  \n",
       "477  The assignment is well-written and includes a ...  \n",
       "479  Thank you for your well-detailed approach to r...  \n",
       "482  Thank you for your submission. The detailed un...  \n",
       "483  Your assignment showcases an excellent underst...  \n",
       "\n",
       "[427 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_csv(\"with_openai_generated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = {\"description\":25, \"change\":20, \"method\":20, \"readable\":25, \"pushed changes to github\":10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GPT4All(model=\"/home/hdu5/.local/share/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
    "# gpt4all-falcon-newbpe-q4_0.gguf     Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\n",
    "# localdocs_v1.db                     test_write.txt\n",
    "# Meta-Llama-3-8B-Instruct.Q4_0.gguf\n",
    "\n",
    "llm2 = GPT4All(model=\"/home/hdu5/.local/share/nomic.ai/GPT4All/gpt4all-falcon-newbpe-q4_0.gguf\")\n",
    "llm3 = GPT4All(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Here is your rubric:\n",
      "\n",
      "| Term | Points | Description |\n",
      "| --- | --- | --- |\n",
      "| description | 25 | The clarity and completeness of the project's description. Is it easy to understand what the project does? Are all necessary details included? |\n",
      "| change | 20 | The ability to make changes to the codebase, including adding new features or fixing bugs. Can you modify existing code without introducing errors? |\n",
      "| method | 20 | The approach taken in implementing the solution. Is it efficient and effective? Does it follow best practices for software development? |\n",
      "| readable | 25 | The readability of the code. Are variable names descriptive, comments are helpful, and formatting is consistent? Can someone else easily understand your code? |\n",
      "| pushed changes to github | 10 | Has the project been successfully deployed to GitHub with a clear commit history and meaningful commit messages? |\n",
      "\n",
      "Please note that this rubric assesses various aspects of software development, including documentation, coding skills, and collaboration. It can be used as a starting point for evaluating projects or codebases in general.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Your task is to generate a rubric from the given keywords and related points. Please generate the rubric formatted as this format: | Term | Points | Description |. The follwing are the keywords and points\"+str(selected_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure, here is an example of a rubric that could be used to evaluate student work based on the given keywords:\n",
      "| Keyword | Points | Description |\n",
      "|---------|--------|--------------|\n",
      "| Creativity | 20%   | The level of originality and inventiveness in the design or execution of the project. |\n",
      "| Organization | 15%   | How well the project is structured, with clear headings, subheadings, and a logical flow of information. |\n",
      "| Clarity | 20%    | The effectiveness of the language used to convey ideas clearly and concisely, without ambiguity or confusion. |\n",
      "| Accuracy | 15%   | How well the content is accurate and free from errors in grammar, spelling, punctuation, and factual information. |\n",
      "| Completeness | 20% | The extent to which all required elements are included in the project, with no important details missing or neglected. |\n"
     ]
    }
   ],
   "source": [
    "print('\\nSure, here is an example of a rubric that could be used to evaluate student work based on the given keywords:\\n| Keyword | Points | Description |\\n|---------|--------|--------------|\\n| Creativity | 20%   | The level of originality and inventiveness in the design or execution of the project. |\\n| Organization | 15%   | How well the project is structured, with clear headings, subheadings, and a logical flow of information. |\\n| Clarity | 20%    | The effectiveness of the language used to convey ideas clearly and concisely, without ambiguity or confusion. |\\n| Accuracy | 15%   | How well the content is accurate and free from errors in grammar, spelling, punctuation, and factual information. |\\n| Completeness | 20% | The extent to which all required elements are included in the project, with no important details missing or neglected. |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", and customer?\\nSure! Here's an example of a rubric that could be used in a graduate-level class related to services and customers:\\n| Category | Description | Points |\\n| --- | --- | --- |\\n| Introduction | Clearly states the purpose and scope of the project, including key terms and definitions. | 20% |\\n| Literature Review | Provides an overview of relevant literature on the topic, including both theoretical and empirical sources. Discusses how this research contributes to existing knowledge in the field. | 30% |\\n| Methodology | Outlines the methods used to conduct the study, including any data collection or analysis techniques. Clearly explains why these methods were chosen and how they will contribute to answering the research question(s). | 25% |\\n| Results | Presents findings from the data analysis in a clear and concise manner. Discusses any patterns or trends that emerged, and how these relate to the literature reviewed earlier. | 30% |\\n| Conclusion | Summarizes the main points of the study and their implications for future research. Reflects on the limitations of the study and\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('Could you generate a rubric for graduate level class with the given keywords: example, service, case, service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and customer?\n",
      "Sure! Here's an example of a rubric that could be used in a graduate-level class related to services and customers:\n",
      "| Category | Description | Points |\n",
      "| --- | --- | --- |\n",
      "| Introduction | Clearly states the purpose and scope of the project, including key terms and definitions. | 20% |\n",
      "| Literature Review | Provides an overview of relevant literature on the topic, including both theoretical and empirical sources. Discusses how this research contributes to existing knowledge in the field. | 30% |\n",
      "| Methodology | Outlines the methods used to conduct the study, including any data collection or analysis techniques. Clearly explains why these methods were chosen and how they will contribute to answering the research question(s). | 25% |\n",
      "| Results | Presents findings from the data analysis in a clear and concise manner. Discusses any patterns or trends that emerged, and how these relate to the literature reviewed earlier. | 30% |\n",
      "| Conclusion | Summarizes the main points of the study and their implications for future research. Reflects on the limitations of the study and\n"
     ]
    }
   ],
   "source": [
    "print(\", and customer?\\nSure! Here's an example of a rubric that could be used in a graduate-level class related to services and customers:\\n| Category | Description | Points |\\n| --- | --- | --- |\\n| Introduction | Clearly states the purpose and scope of the project, including key terms and definitions. | 20% |\\n| Literature Review | Provides an overview of relevant literature on the topic, including both theoretical and empirical sources. Discusses how this research contributes to existing knowledge in the field. | 30% |\\n| Methodology | Outlines the methods used to conduct the study, including any data collection or analysis techniques. Clearly explains why these methods were chosen and how they will contribute to answering the research question(s). | 25% |\\n| Results | Presents findings from the data analysis in a clear and concise manner. Discusses any patterns or trends that emerged, and how these relate to the literature reviewed earlier. | 30% |\\n| Conclusion | Summarizes the main points of the study and their implications for future research. Reflects on the limitations of the study and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
